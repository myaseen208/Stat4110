[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat-4110: Statistical Methods for Process Development and Control",
    "section": "",
    "text": "Preface\nThis booklet provides the class notes for Stat-4110: Statistical Methods for Process Development and Control, based on Engineering Statistics by Montgomery, Runger, and Hubele (please refer to the textbook for complete and comprehensive details). The following topics are covered:\n\nThe Role of Statistics in Engineering\nData Summary and Presentation\nRandom Variables and Probability Distributions\nDecision Making for a Single Sample\nDecision Making for Two Samples\nBuilding Empirical Models\nStatistical Process Control\n\nMontgomery, D. C., Runger, G. C., & Hubele, N. F. (2014). Engineering statistics. Wiley.\n\n\n\n\n\n\n\nFigure 1: Spring 2025 Stat-4110 Syllabus",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Ch01.html",
    "href": "Ch01.html",
    "title": "1  The Role of Statistics in Engineering",
    "section": "",
    "text": "1.1 The Engineering Method and Statistical Thinking",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Role of Statistics in Engineering</span>"
    ]
  },
  {
    "objectID": "Ch01.html#the-engineering-method-and-statistical-thinking",
    "href": "Ch01.html#the-engineering-method-and-statistical-thinking",
    "title": "1  The Role of Statistics in Engineering",
    "section": "",
    "text": "Engineers solve problems of interest to society by the efficient application of scientific principles.\nThe engineering or scientific method is the approach to formulating and solving these problems.\n\n\n\n\n\n\n\nFigure 1.1: The engineering problem-solving method.\n\n\n\n\n1.1.1 Probability\n\n\n\n\n\n\nProbability\n\n\n\n\nUsed to quantify likelihood or chance\nUsed to represent risk or uncertainty in engineering applications\nCan be interpreted as our degree of belief or relative frequency\n\n\n\n\n\n1.1.2 Statistics\n\n\n\n\n\n\nStatistics\n\n\n\n\nDeals with the collection, presentation, analysis, and use of data:\n\nMake decisions\nSolve problems\nDesign products and processes\n\nStatistical techniques are useful for describing and understanding variability.\nBy variability, we mean successive observations of a system or phenomenon do not produce exactly the same result.\nStatistics gives us a framework for describing this variability and for learning about potential sources of variability.\n\n\n\n\n\n1.1.3 Statistical Reasoning\n\n\n\n\n\n\nStatistical Reasoning\n\n\n\n\nStatistics is the science of uncertainty & variability\nTurning Data into Information\n\nData → Information → Knowledge → Wisdom\n\n\n\n\n\n\n\n\nFigure 1.2: Big Data\n\n\n\n\nStatistics is the Art and Science of learning from Data.\n\n\n\n\n\n1.1.4 Definitions\n\n\n\n\n\n\nDefinitions\n\n\n\n\nPopulation\n\nSet of measurements of interest. Characteristics of the population (parameters) are typically of interest.\n\nSample\n\nSubset of measurements of interest. A characteristic of the sample (statistic) is used to infer population characteristics (parameters).\n\nParameter\n\nA characteristic of the population.\n\nStatistic\n\nA characteristic of the sample.\n\nDescriptive Statistics\n\nDescribing the important characteristics of a set of data.\n\nInferential Statistics\n\nUsing sample data to make inferences (or generalizations) about a population.\n\nStatistical Inference\n\nMaking a statement about the population (parameter) based on the sample (statistic).\n\n\n\n\n\n\n1.1.5 Example\n\n\n\n\n\n\nExample\n\n\n\nSuppose that an engineer is developing a rubber compound for use in O-rings. The O-rings are to be employed as seals in plasma etching tools used in the semiconductor industry, so their resistance to acids and other corrosive substances is an important characteristic. The engineer uses the standard rubber compound to produce eight O-rings in a development laboratory and measures the tensile strength of each specimen after immersion in a nitric acid solution at 30°C for 25 minutes [refer to the American Society for Testing and Materials (ASTM) Standard D 1414 and the associated standards for many interesting aspects of testing rubber O-rings]. The tensile strengths (in psi) of the eight O-rings are 1030, 1035, 1020, 1049, 1028, 1026, 1019, and 1010. As we should have anticipated, not all the O-ring specimens exhibit the same measurement of tensile strength.\n\n\n\n\n1.1.6 Random Variable\n\n\n\n\n\n\nRandom Variable\n\n\n\n\nSince tensile strength varies or exhibits variability, it is a random variable.\nA random variable, X, can be model by X = \\mu + \\epsilon  where \\mu is a constant and \\epsilon is a random disturbance, or “noise” term.\n\n\n\n\n\n1.1.7 Dot Diagram\n\n\n\n\n\n\nDot Diagram\n\n\n\n\nThe dot diagram is a very useful plot for displaying a small body of data - say up to about 20 observations.\nThis plot allows us to see easily two features of the data; the location, or the middle, and the scatter or variability.\n\n\n\n\n\n\n\nFigure 1.3: Dot diagram of the O-ring tensile strength data for the original rubber compound.\n\n\n\n\nThe dot diagram is also very useful for comparing sets of data.\n\n\n\n\n\n\n\nFigure 1.4: Dot diagram of O-ring tensile strength data for the original and modified rubber compounds.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Role of Statistics in Engineering</span>"
    ]
  },
  {
    "objectID": "Ch01.html#collecting-engineering-data",
    "href": "Ch01.html#collecting-engineering-data",
    "title": "1  The Role of Statistics in Engineering",
    "section": "1.2 Collecting Engineering Data",
    "text": "1.2 Collecting Engineering Data\n\n\n\n\n\n\nCollecting Engineering Data\n\n\n\nThree basic methods for collecting data:\n\nA retrospective study using historical data\nAn observational study\nA designed experiment\n\n\n\n\n1.2.1 Retrospective Study\n\n\n\n\n\n\nRetrospective Study\n\n\n\nA retrospective study uses either all or a sample of the historical process data from some period of time. The objective of this study might be to determine the relationships among the two temperatures and the reflux rate on the acetone concentration in the output product stream. In most such studies, the engineer is interested in using the data to construct a model relating the variables of interest.\n\n\n\n\n1.2.2 Observational Study\n\n\n\n\n\n\nObservational Study\n\n\n\nAn observational study simply observes the process of population during a period of routine operation.\n\n\n\n\n1.2.3 Designed Experiments\n\n\n\n\n\n\nDesigned Experiments\n\n\n\nThe third way that engineering data are collected is with a designed experiment. In a designed experiment, the engineer makes deliberate or purposeful changes in controllable variables (called factors) of the system, observes the resulting system output, and then makes a decision or an inference about which variables are responsible for the changes that he or she observes in the output performance.\n\n\n\n\n1.2.4 Random Samples\n\n\n\n\n\n\nRandom Samples\n\n\n\n\nAlmost all statistical analysis is based on the idea of using a sample of data that has been selected from some population.\nThe objective is to use the sample data to make decisions or learn something about the population.\nOnly random samples are likely to be useful in statistics, as they give us the best chance of obtaining a sample that is representative of the population.\n\n\n\n\n\n\n\n\n\nSimple Random Sample\n\n\n\n\nA simple random sample of size n is a sample that has been selected from a population in such a way that each possible sample of size n has an equally likely chance of being selected.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Role of Statistics in Engineering</span>"
    ]
  },
  {
    "objectID": "Ch01.html#observing-processes-over-time",
    "href": "Ch01.html#observing-processes-over-time",
    "title": "1  The Role of Statistics in Engineering",
    "section": "1.3 Observing Processes over Time",
    "text": "1.3 Observing Processes over Time\n\n\n\n\n\n\nObserving Processes over Time\n\n\n\n\nWhenever data are collected over time it is important to plot the data over time. Phenomena that might affect the system or process often become more visible in a time-oriented plot and the concept of stability can be better judged.\n\n\n\n\n\n\n\nFigure 1.5: A dot diagram illustrates variation but does not identify the problem.\n\n\n\n\n\n\n\n\n\nFigure 1.6: A time series plot of acetone concentration provides more information than the dot diagram.\n\n\n\n\n\n\n\n\n\nFigure 1.7: A control chart for the chemical process concentration data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Role of Statistics in Engineering</span>"
    ]
  },
  {
    "objectID": "Ch01.html#r-software",
    "href": "Ch01.html#r-software",
    "title": "1  The Role of Statistics in Engineering",
    "section": "1.4 R Software",
    "text": "1.4 R Software\n\n\n\n\n\n\nDefinition\n\n\n\nR is a free, open-source programming language and software environment for statistical computing, bioinformatics, visualization and general computing. R provides a wide variety of statistical and graphical techniques, and is highly extensible. The latest version of R can be obtained from https://cran.r-project.org/bin/.\n\n\n\n1.4.1 RStudio\nRStudio is a powerful integrated development environment (IDE) for R, and it can be downloaded from https://www.rstudio.com/products/rstudio/download/.\n\n\n1.4.2 Example Code (Minimal)\n\nCh01.RCh01.qmd\n\n\n\n2 + 2\n5 - 2\n5 * 2\n6/2\n\n\n\n\n---\ntitle: \"Statistics and the Scientific Method\"\nformat:\n  html\ntoc: true\nnumber-sections: true\n---\n\n# Introduction\n\n```{r}\n#| echo: true\n2 + 2\n5 - 2\n5 * 2\n6/2 \n```\n\n\n\n\n\n\n1.4.3 An Introduction to R\n\n\n\n\n\n\n\nFigure 1.8: An Introduction to R\n\n\n\n\n\n1.4.4 R Short Reference Card\n\n\n\n\n\n\n\nFigure 1.9: R Short Reference Card\n\n\n\n\n\n1.4.5 2009 New York Times Article on R\n\n\n\n\n\n\n\nFigure 1.10: 2009 New York Times Article on R\n\n\n\n\n\n1.4.6 2015 Nature Article on R\n\n\n\n\n\n\n\nFigure 1.11: 2015 Nature Article on R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Role of Statistics in Engineering</span>"
    ]
  },
  {
    "objectID": "Ch02.html",
    "href": "Ch02.html",
    "title": "2  Data Summary and Presentation",
    "section": "",
    "text": "2.1 Data Summary and Display",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Summary and Presentation</span>"
    ]
  },
  {
    "objectID": "Ch02.html#data-summary-and-display",
    "href": "Ch02.html#data-summary-and-display",
    "title": "2  Data Summary and Presentation",
    "section": "",
    "text": "2.1.1 Mean\n\n2.1.1.1 Sample Mean\n\n\n\n\n\n\nSample Mean\n\n\n\nIf the n observations in a sample are denoted by X_{1},X_{2},\\ldots,X_{n}, the sample mean is \\begin{aligned}\n\\overline{X} & = \\frac{X_{1} + X_{2} + \\cdots + X_{n}}{n} \\\\\n\\overline{X} & = \\frac{\\sum_{i=1}^{n} X_{i}}{n}\n\\end{aligned}\n\n\n\n\n2.1.1.2 Population Mean\n\n\n\n\n\n\nPopulation Mean\n\n\n\nWhen there is a finite number of observations (say, N) in the population, the population mean is \\begin{aligned}\n\\mu & = \\frac{\\sum_{i=1}^{N} X_{i}}{N}\n\\end{aligned}\n\n\nThe sample mean, \\overline{X}, is a reasonable estimate of the population mean, \\mu.\n\n\n2.1.1.3 Example\n\n\n\n\n\n\nExample\n\n\n\nConsider the O-ring tensile strength experiment described in Chapter 1. The data from the modified rubber compound (1048, 1059, 1047, 1066, 1040, 1070, 1037, 1073) are shown in the dot diagram (Figure 2.1 and Figure 2.2). The sample mean strength (psi) for the eight observations on strength is \\begin{aligned}\n\\overline{X} & = \\frac{\\sum_{i=1}^{n} X_{i}}{n} \\\\\n\\overline{X} & = \\frac{1037 + 1047 + \\cdots + 1040}{8} \\\\\n\\overline{X} & = \\frac{8440}{8} = 1055  \\text{ psi}\n\\end{aligned}\nA physical interpretation of the sample mean as a measure of location is shown in Figure 2.1 and Figure 2.2. Note that the sample mean \\overline{X} = 1055 can be thought of as a “balance point.” That is, if each observation represents 1 pound of mass placed at the point on the x-axis, a fulcrum located at \\overline{X} would exactly balance this system of weights.\n\n\n\n\n\n\nFigure 2.1: Dot diagram of O-ring tensile strength. The sample mean is shown as a balance point for a system of weights.\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] 1055\n\n\n\n\n\nlibrary(fastverse)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(janitor)\nlibrary(kableExtra)\n\n\nExmp2.2 &lt;- \n  data.table(\n    TS = c(1048, 1059, 1047, 1066, 1040, 1070, 1037, 1073)\n    )\n\nExmp2.2 %&gt;% \n  pull(TS) %&gt;% \n  fmean(x = .)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: Dot diagram of O-ring tensile strength. The sample mean is shown as a balance point for a system of weights.\n\n\n\n\n\n\n\n\n# Dot diagram of O-ring tensile strength. The sample mean is shown as a balance point for a system of weights.\nggplot(data = Exmp2.2, mapping = aes(x = TS)) +\n  geom_dotplot(binwidth = 1, stackdir = \"up\", dotsize = 1) +\n  geom_vline(aes(xintercept = fmean(TS)), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  labs(\n    x = \"Tensile Strength\"\n  , y = NULL\n  ) +\n  theme_classic() +\n  theme(\n    axis.ticks.y = element_blank()\n  , axis.text.y = element_blank() \n  )\n\n\n\n\n\n\n\n\n\n2.1.2 Variance and Standard Deviation\n\n2.1.2.1 Sample Variance and Sample Standard Deviation\n\n\n\n\n\n\nSample Variance and Sample Standard Deviation\n\n\n\nIf the n observations in a sample are denoted by X_{1},X_{2},\\ldots,X_{n}, then the sample variance is\n\\begin{aligned}\nS^2 & = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}{n-1}\n\\end{aligned}\nThe sample standard deviation, S, is the positive square root of the sample variance.\n\n\nThe units of measurement for the sample variance are the square of the original units of the variable. Thus, if X is measured in psi, the units for the sample variance are (\\text{psi})^2. The standard deviation has the desirable property of measuring variability in the original units of the variable of interest, X (psi).\n\n\n2.1.2.2 Example\n\n\n\n\n\n\nExample\n\n\n\nConsider the O-ring tensile strength experiment described in Chapter 1. The data from the modified rubber compound (1048, 1059, 1047, 1066, 1040, 1070, 1037, 1073) are shown in the dot diagram (Figure 2.3). The numerator of S^2 is (See Table 2.1) \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2} = 1348\nso the sample variance is\n\\begin{aligned}\nS^2 & = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}{n-1}\n= \\frac{1348}{8-1} = 192.57 \\text{ psi}^2 \\end{aligned}\nand the sample standard deviation is\n\\begin{aligned}\nS & = \\sqrt{192.57} = 13.9 \\text{ psi} \\end{aligned}\n\n\n\nTable 2.1: Calculation of Terms for the Sample Variance and Sample Standard Deviation\n\n\n\n\n\n\n\n\n\n\n\ni\nX_i\nX_{i}-\\overline{X}\n\\left(X_{i}-\\overline{X}\\right)^{2}\n\n\n\n\n1\n1,048\n-7\n49\n\n\n2\n1,059\n4\n16\n\n\n3\n1,047\n-8\n64\n\n\n4\n1,066\n11\n121\n\n\n5\n1,040\n-15\n225\n\n\n6\n1,070\n15\n225\n\n\n7\n1,037\n-18\n324\n\n\n8\n1,073\n18\n324\n\n\nTotal\n8,440\n0\n1,348\n\n\n\n\n\n\nAlso note that \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}=\\sum_{i=1}^{n}X_{i}^{2}-\\frac{\\left(\\sum_{i=1}^{n}X_{i}\\right)^{2}}{n}.\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] 192.5714\n\n\n[1] 13.87701\n\n\n\n\n\nExmp2.2 %&gt;% \n  pull(TS) %&gt;% \n  fvar(x = .)\n\nExmp2.2 %&gt;% \n  pull(TS) %&gt;% \n  fsd(x = .)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\nTable 2.2: Variance Calculations\n\n\n\n\n\n\nTS\nDev\nDev2\n\n\n\n\n1048\n-7\n49\n\n\n1059\n4\n16\n\n\n1047\n-8\n64\n\n\n1066\n11\n121\n\n\n1040\n-15\n225\n\n\n1070\n15\n225\n\n\n1037\n-18\n324\n\n\n1073\n18\n324\n\n\nTotal\n0\n1348\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.3\n\n\nExmp2.2 %&gt;% \n  fmutate(\n    Dev  = TS - fmean(TS)\n  , Dev2 = (TS - fmean(TS))^2\n    ) %&gt;% \n  adorn_totals() %&gt;% \n  kbl()\n\n\n\n\n\n\n\n\n\n\n2.1.2.3 How Does the Sample Variance Measure Variability?\n\n\n\n\n\n\nHow Does the Sample Variance Measure Variability?\n\n\n\nTo see how the sample variance measures dispersion or variability, refer to Figure 2.3, which shows the deviations X_{i}-\\overline{X} for the O-ring tensile strength data. The greater the amount of variability in the O-ring tensile strength data, the larger in absolute magnitude some of the deviations X_{i}-\\overline{X} will be. Because the deviations X_{i}-\\overline{X} always sum to zero, we must use a measure of variability that changes the negative deviations to nonnegative quantities. Squaring the deviations is the approach used in the sample variance. Consequently, if S^2 is small, there is relatively little variability in the data, but if S^2 is large, the variability is relatively large.\n\n\n\n\n\n\nFigure 2.3: How the sample variance measures variability through the deviations X_{i}-\\overline{X}.\n\n\n\n\n\n\n\n2.1.2.4 Population Variance\n\n\n\n\n\n\nPopulation Variance\n\n\n\nWhen the population is finite and consists of N values, we may define the population variance as\n\\begin{aligned}\n\\sigma^2 & = \\frac{\\sum_{i=1}^{N}\\left(X_{i}-\\mu\\right)^{2}}{N}\n\\end{aligned}\n\n\nThe sample variance, S^2, is a reasonable estimate of the population variance, \\sigma^2.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Summary and Presentation</span>"
    ]
  },
  {
    "objectID": "Ch02.html#stem-and-leaf-diagram",
    "href": "Ch02.html#stem-and-leaf-diagram",
    "title": "2  Data Summary and Presentation",
    "section": "2.2 Stem-and-Leaf Diagram",
    "text": "2.2 Stem-and-Leaf Diagram\n\n\n\n\n\n\nStem-and-Leaf Diagram\n\n\n\nA stem-and-leaf diagram is a good way to obtain an informative visual display of a data set X_{1},X_{2},\\ldots,X_{n}, where each number X_{i} consists of at least two digits. To construct a stem-and-leaf diagram, use the following steps:\n\n\n\n\n\n\nSteps for Constructing a Stem-and-Leaf Diagram\n\n\n\n\nDivide each number X_{i} into two parts: a stem, consisting of one or more of the leading digits, and a leaf, consisting of the remaining digit.\nList the stem values in a vertical column.\nRecord the leaf for each observation beside its stem.\nWrite the units for stems and leaves on the display.\n\n\n\n\n\n\n2.2.1 Example\n\n\n\n\n\n\nExample\n\n\n\nTo illustrate the construction of a stem-and-leaf diagram, consider the alloy compressive strength data in Table 2.4.\n\n\n\nTable 2.4: Compressive Strength of 80 Aluminum-Lithium Alloy Specimens\n\n\n\n\n\n105\n221\n183\n186\n121\n181\n180\n143\n\n\n97\n154\n153\n174\n120\n168\n167\n141\n\n\n245\n228\n174\n199\n181\n158\n176\n110\n\n\n163\n131\n154\n115\n160\n208\n158\n133\n\n\n207\n180\n190\n193\n194\n133\n156\n123\n\n\n134\n178\n76\n167\n184\n135\n229\n146\n\n\n218\n157\n101\n171\n165\n172\n158\n169\n\n\n199\n151\n142\n163\n145\n171\n148\n158\n\n\n160\n175\n149\n87\n160\n237\n150\n135\n\n\n196\n201\n200\n176\n150\n170\n118\n149\n\n\n\n\n\n\nWe will select as stem values the numbers 7, 8, 9, \\ldots, 24. The resulting stem-and-leaf diagram is presented in the following figure.\n\nR OutputR CodeDownload Data\n\n\n\n\n\nTable 2.5: Stem-and-leaf diagram for the compressive strength data\n\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   7 | 6\n   8 | 7\n   9 | 7\n  10 | 15\n  11 | 058\n  12 | 013\n  13 | 133455\n  14 | 12356899\n  15 | 001344678888\n  16 | 0003357789\n  17 | 0112445668\n  18 | 0011346\n  19 | 034699\n  20 | 0178\n  21 | 8\n  22 | 189\n  23 | 7\n  24 | 5\n\n\n\n\n\n\n\n\n\nTable 2.6\n\n\nExmp2.4 &lt;- fread(\"./data/Exmp2.4.csv\")\nExmp2.4 %&gt;% \n  pull(CS) %&gt;% \n  stem(x = ., scale = 2, width = 80)\n\n\n\n\n\n\n\n\n\nPractical interpretation: Inspection of this display immediately reveals that most of the compressive strengths lie between 110 and 200 psi and that a central value is somewhere between 150 and 160 psi. Furthermore, the strengths are distributed approximately symmetrically about the central value. The stem-and-leaf diagram enables us to determine quickly some important features of the data that were not immediately obvious in the original display in the table.\n\nR OutputR Code\n\n\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   76.0   144.5   161.5   162.7   181.0   245.0 \n\n\n\n\n\nExmp2.4 %&gt;% \n  pull(CS) %&gt;% \n  summary()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Summary and Presentation</span>"
    ]
  },
  {
    "objectID": "Ch02.html#histograms",
    "href": "Ch02.html#histograms",
    "title": "2  Data Summary and Presentation",
    "section": "2.3 Histograms",
    "text": "2.3 Histograms\n\n2.3.1 Histogram\n\n\n\n\n\n\nHistogram\n\n\n\nA histogram is a graphical representation of the distribution of a dataset, used to summarize and visualize the frequency or relative frequency of data within specified intervals, called bins. Each bin represents a range of values, and the height of the bar corresponding to a bin reflects the number of data points (or proportion of data points) that fall within that range. Histograms are particularly useful for understanding the shape, central tendency, spread, and outliers in a dataset, especially for continuous or large datasets.\n\n\n\n\n2.3.2 Example\n\n\n\n\n\n\nGold Ball Distance\n\n\n\nThe United States Golf Association tests golf balls to ensure that they conform to the rules of golf. Balls are tested for weight, diameter, roundness, and conformance to an overall distance standard. The overall distance test is conducted by hitting balls with a driver swung by a mechanical device nicknamed Iron Byron, after the legendary great player Byron Nelson, whose swing the machine is said to emulate.\n\n\n\nTable 2.7: Golf Ball Distance Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n291.5\n274.4\n290.2\n276.4\n272.0\n268.7\n281.6\n281.6\n276.3\n285.9\n\n\n269.6\n266.6\n283.6\n269.6\n277.8\n287.8\n267.6\n292.6\n273.4\n284.4\n\n\n270.7\n274.0\n285.2\n275.5\n272.1\n261.3\n274.0\n279.3\n281.0\n293.1\n\n\n277.5\n278.0\n272.5\n271.7\n280.8\n265.6\n260.1\n272.5\n281.3\n263.0\n\n\n279.0\n267.3\n283.5\n271.2\n268.5\n277.1\n266.2\n266.4\n271.5\n280.3\n\n\n267.8\n272.1\n269.7\n278.5\n277.3\n280.5\n270.8\n267.7\n255.1\n276.4\n\n\n283.7\n281.7\n282.2\n274.1\n264.5\n281.0\n273.2\n274.4\n281.6\n273.7\n\n\n271.0\n271.5\n289.7\n271.1\n256.9\n274.5\n286.2\n273.9\n268.5\n262.6\n\n\n261.9\n258.9\n293.2\n267.1\n255.0\n269.7\n281.9\n269.6\n279.8\n269.9\n\n\n282.6\n270.0\n265.2\n277.7\n275.5\n272.2\n270.0\n271.0\n284.3\n268.4\n\n\n\n\n\n\nTable 2.7 gives the distances achieved (in yards) by hitting 100 golf balls of a particular brand in the overall distance test. Because the data set contains 100 observations and \\sqrt{100} = 10, we suspect that about 10 bins will provide a satisfactory histogram. The histogram for the golf ball distance data is shown in Figure 2.4 (a). Notice that the midpoint of the first bin is 250 yards and that the histogram only has 9 bins that contain a nonzero frequency. A histogram, like a stem-and-leaf plot, gives a visual impression of the shape of the distribution of the measurements, as well as information about the inherent variability in the data. Note the reasonably symmetric or bell-shaped distribution of the golf ball distance data.\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram with 10 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram with 16 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) A cumulative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) A relative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\nFigure 2.4: Histogram for the golf ball distance data\n\n\n\n\n\n\nExmp2.6 &lt;- fread(\"./data/Exmp2.6.csv\")\n\n# Histogram with 10 bins for the golf ball distance data\nggplot(data = Exmp2.6, mapping = aes(x = Distance)) +\n  geom_histogram(bins = 9) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), breaks = pretty_breaks(n = 8)) +\n  labs(\n    x = \"Distance\"\n  , y = \"Frequency\"\n  ) +\n  theme_classic()\n\n# Histogram with 16 bins for the golf ball distance data\nggplot(data = Exmp2.6, mapping = aes(x = Distance)) +\n  geom_histogram(bins = 16) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), breaks = pretty_breaks(n = 8)) +\n  labs(\n    x = \"Distance\"\n  , y = \"Frequency\"\n  ) +\n  theme_classic()\n\n# A cumulative frequency plot of the golf ball distance data\nggplot(data = Exmp2.6, mapping = aes(x = Distance)) +\n  geom_histogram(mapping = aes(y = after_stat(count / sum(count))), bins = 9) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), breaks = pretty_breaks(n = 8)) +\n  labs(\n    x = \"Distance\"\n  , y = \"Frequency\"\n  ) +\n  theme_classic()\n\n# A relative frequency plot of the golf ball distance data\nggplot(data = Exmp2.6, mapping = aes(x = Distance)) +\n  geom_histogram(mapping = aes(y = after_stat(cumsum(count))), bins = 10) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), breaks = pretty_breaks(n = 8)) +\n  labs(\n    x = \"Distance\"\n  , y = \"Frequency\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n2.3.3 Pareto Chart\n\n\n\n\n\n\nPareto Chart\n\n\n\nAn important variation of the histogram is the Pareto chart. This chart is widely used in quality and process improvement studies where the data usually represent different types of defects, failure modes, or other categories of interest to the analyst. The categories are ordered so that the category with the largest number of frequencies is on the left, followed by the category with the second largest number of frequencies, and so forth.\n\n\n\n\n2.3.4 Example\n\n\n\n\n\n\nExample\n\n\n\nTable 2.8 presents data on aircraft accident rates taken from an article in The Wall Street Journal (“Jet’s Troubled History Raises Issues for the FAA and the Manufacturer,” 19 September 2000). The table presents the total number of accidents involving hull losses between 1959 and 1999 for 22 types of aircraft and the hull loss rate expressed as the number of hull losses per million departures. Figure 2-11 shows a Pareto chart of the hull losses per million departures. Clearly, the first three aircraft types account for a large percentage of the incidents on a per-million-departures basis. An interesting fact about the first three aircraft types is that the 707/720 and the DC-8 were mid-1950s’ designs and are not in regular\n\n\n\nTable 2.8: Aircraft Accident Data\n\n\n\n\n\n\n\n\n\n\nAircraft Type\nActual Number of Hull Losses\nHull Losses/Million Departures\n\n\n\n\nMD-11\n5\n6.54\n\n\n707/720\n115\n6.46\n\n\nDC-8\n71\n5.84\n\n\nF-28\n32\n3.94\n\n\nBAC 1-11\n22\n2.64\n\n\nDC-10\n20\n2.57\n\n\n747-Early\n21\n1.9\n\n\nA310\n4\n1.4\n\n\nA300-600\n3\n1.34\n\n\nDC-9\n75\n1.29\n\n\nA300-Early\n7\n1.29\n\n\n737-1 & 2\n62\n1.23\n\n\n727\n70\n0.97\n\n\nA310/319/321\n7\n0.96\n\n\nF100\n3\n0.8\n\n\nL-1011\n4\n0.77\n\n\nBae 146\n3\n0.59\n\n\n747-400\n1\n0.49\n\n\n757\n4\n0.46\n\n\nMD-80/90\n10\n0.43\n\n\n767\n3\n0.41\n\n\n737-3,4 & 5\n12\n0.39\n\n\n\n\n\n\npassenger service today in most of the world, whereas the MD-11 was introduced into passenger service in 1990. Between 1990 and 1999, five of the global fleet of 198 MD-11s were destroyed in crashes, leading to the high accident rate (an excellent discussion of potential root causes of these accidents is in The Wall Street Journal article). The purpose of most Pareto charts is to help the analyst separate the sources of defects or incidents into the vital few and the relatively “insignificant many.” There are many varia- tions of the Pareto chart; for some examples, see Montgomery (2009a).\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 2.5: Pareto chart for the aircraft accident data\n\n\n\n\n\n\n\n\nExmp2.8 &lt;- fread(\"./data/Exmp2.8.csv\")\n\nggplot(data = Exmp2.8, mapping = aes(x = fct_reorder(Type, desc(HLMD)), y = HLMD)) +\n  geom_col() +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), breaks = pretty_breaks(n = 8)) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  labs(\n    x = \"Aircraft type\"\n  , y = \"Hull losses/million departures\"\n  ) +\n  theme_classic()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Summary and Presentation</span>"
    ]
  },
  {
    "objectID": "Ch02.html#box-plot",
    "href": "Ch02.html#box-plot",
    "title": "2  Data Summary and Presentation",
    "section": "2.4 Box Plot",
    "text": "2.4 Box Plot\n\n\n\n\n\n\nBox Plot\n\n\n\nBox plot or Box Whisker plot can be used to pick out pertinent aspects of data such as\n\nMeasure of Central Tendency\nMeasure of Variability\nMeasure of Symmetry\nOutliers\nGroup Comparisons\n\n\n Box Whisker Plot\n\n\n\\begin{align*}\n\\text{Lower Inner Fence} & =Q_{1}-1.5\\times\\text{IQR}\\\\\n\\text{Upper Inner Fence} & =Q_{3}+1.5\\times\\text{IQR}\\\\\n\\text{Lower Outer Fence} & =Q_{1}-3\\times\\text{IQR}\\\\\n\\text{Upper Outer Fence} & =Q_{3}+3\\times\\text{IQR}\n\\end{align*}\n\n\nMild Outlier: Beyond inner fence (but inside outer fence)\nExtreme Outlier: Beyond outer fence\nNote that the lines from the boxes extend to the lower and upper adjacent values (the smallest and largest values that are not outliers).\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 2.6: Box plot for compressive strength data\n\n\n\n\n\n\n\n\nExmp2.4 &lt;- fread(\"./data/Exmp2.4.csv\")\n\nggplot(data = Exmp2.4, mapping = aes(x = CS)) +\n  geom_boxplot(width = 0.3) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(limits = c(-1, 1)) +\n  labs(\n    x = \"Strength\"\n  , y = NULL\n  ) +\n  theme_classic() +\n  theme(\n    axis.ticks.y = element_blank()\n  , axis.text.y = element_blank() \n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.7: Comparative box plots of a quality index at three plants.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Summary and Presentation</span>"
    ]
  },
  {
    "objectID": "Ch02.html#time-series-plots",
    "href": "Ch02.html#time-series-plots",
    "title": "2  Data Summary and Presentation",
    "section": "2.5 Time Series Plots",
    "text": "2.5 Time Series Plots\n\n\n\n\n\n\nTime Series Plots\n\n\n\n\nA time series or time sequence is a data set in which the observations are recorded in the order in which they occur.\nA time series plot is a graph in which the vertical axis denotes the observed value of the variable (say x) and the horizontal axis denotes the time (which could be minutes, days, years, etc.).\nWhen measurements are plotted as a time series, we often see\n\ntrends,\ncycles, or\nother broad features of the data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Company sales by year\n\n\n\n\n\n\n\n\n\n\n\n(b) Company sales by quarter\n\n\n\n\n\n\n\nFigure 2.8: Company sales\n\n\n\n\n\n\n\n\n\nFigure 2.9: A digidot plot of the compressive strength data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Summary and Presentation</span>"
    ]
  },
  {
    "objectID": "Ch02.html#multivariate-data",
    "href": "Ch02.html#multivariate-data",
    "title": "2  Data Summary and Presentation",
    "section": "2.6 Multivariate Data",
    "text": "2.6 Multivariate Data\n\n\n\n\n\n\nMultivariate Data\n\n\n\n\nThe dot diagram, stem-and-leaf diagram, histogram, and box plot are descriptive displays for univariate data; that is, they convey descriptive information about a single variable.\nMany engineering problems involve collecting and analyzing multivariate data, or data on several different variables.\nIn engineering studies involving multivariate data, often the objective is to determine the relationships among the variables or to build an empirical model.\n\n\n\n\n2.6.1 Example\n\n\n\n\n\n\nWire Bond Data\n\n\n\n\n\n\nTable 2.9: Wire Bond Data\n\n\n\n\n\nObs No\nPull Strength (Y)\nWire Length (X1)\nDie Height (X2)\n\n\n\n\n1\n9.95\n2\n50\n\n\n2\n24.45\n8\n110\n\n\n3\n31.75\n11\n120\n\n\n4\n35.00\n10\n550\n\n\n5\n25.02\n8\n295\n\n\n6\n16.86\n4\n200\n\n\n7\n14.38\n2\n375\n\n\n8\n9.60\n2\n52\n\n\n9\n24.35\n9\n100\n\n\n10\n27.50\n8\n300\n\n\n11\n17.08\n4\n412\n\n\n12\n37.00\n11\n400\n\n\n13\n41.95\n12\n500\n\n\n14\n11.66\n2\n360\n\n\n15\n21.65\n4\n205\n\n\n16\n17.89\n4\n400\n\n\n17\n69.00\n20\n600\n\n\n18\n10.30\n1\n585\n\n\n19\n34.93\n10\n540\n\n\n20\n46.59\n15\n250\n\n\n21\n44.88\n15\n290\n\n\n22\n54.12\n16\n510\n\n\n23\n56.63\n17\n590\n\n\n24\n22.13\n6\n100\n\n\n25\n21.15\n5\n400\n\n\n\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scatter plot for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter plot for Pull strength versus die height\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scatter and box plots for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(d) Scatter and box plots for Pull strength versus die height\n\n\n\n\n\n\n\nFigure 2.10: Scatter diagrams and box plots for the wire bond pull strength data\n\n\n\n\n\n\nTable2.9 &lt;- fread(\"./data/Table2.9.csv\")\n\nggplot(data = Table2.9, mapping = aes(x = X1, y = Y)) +\n  geom_point() +\n  labs(\n    x = \"Length\"\n  , y = \"Strength\"\n  ) +\n  theme_classic()\n\nggplot(data = Table2.9, mapping = aes(x = X2, y = Y)) +\n  geom_point() +\n  labs(\n    x = \"Die height\"\n  , y = \"Strength\"\n  ) +\n  theme_classic()\n\nlibrary(car)\nscatterplot(\n      formula = Y ~ X1\n    , data    = Table2.9\n    , smooth  = FALSE\n    , regLine = FALSE\n    , xlab    = \"Length\"\n    , ylab    = \"Strength\"\n    )\n\nscatterplot(\n      formula = Y ~ X2\n    , data    = Table2.9\n    , smooth  = FALSE\n    , regLine = FALSE\n    , xlab    = \"Die height\"\n    , ylab    = \"Strength\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Sample Correlation Coefficient\n\n\n\n\n\n\nSample Correlation Coefficient\n\n\n\nGiven n pairs of data \\left(X_{1},Y_{1}\\right),\\left(X_{2},Y_{2}\\right),\\ldots,\\left(X_{n},Y_{n}\\right), the sample correlation coefficient r is defined by\n\nr=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)\\left(Y_{i}-\\overline{Y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}\\sum_{i=1}^{n}\\left(Y_{i}-\\overline{Y}\\right)^{2}}}\n\nwhere -1\\leq r\\leq+1.\n\n\n\n\n\n\n\n\n\n\n\n(a) r is near +1\n\n\n\n\n\n\n\n\n\n\n\n(b) r is near -1\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) r is near 0, Y and X are unrelated\n\n\n\n\n\n\n\n\n\n\n\n(d) r is near 0, Y and X are nonlinearly related\n\n\n\n\n\n\n\nFigure 2.11: Scatter diagrams for different values of the sample correlation coefficient r.\n\n\n\n\n\n\n\n2.6.3 Example\n\n\n\n\n\n\nData on Shampoo\n\n\n\n\n\n\nTable 2.10: Data on Shampoo\n\n\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\n6.3\n5.3\n4.8\n3.1\n1\n91\n\n\n4.4\n4.9\n3.5\n3.9\n1\n87\n\n\n3.9\n5.3\n4.8\n4.7\n1\n82\n\n\n5.1\n4.2\n3.1\n3.6\n1\n83\n\n\n5.6\n5.1\n5.5\n5.1\n1\n83\n\n\n4.6\n4.7\n5.1\n4.1\n1\n84\n\n\n4.8\n4.8\n4.8\n3.3\n1\n90\n\n\n6.5\n4.5\n4.3\n5.2\n1\n84\n\n\n8.7\n4.3\n3.9\n2.9\n1\n97\n\n\n8.3\n3.9\n4.7\n3.9\n1\n93\n\n\n5.1\n4.3\n4.5\n3.6\n1\n82\n\n\n3.3\n5.4\n4.3\n3.6\n1\n84\n\n\n5.9\n5.7\n7.2\n4.1\n2\n87\n\n\n7.7\n6.6\n6.7\n5.6\n2\n80\n\n\n7.1\n4.4\n5.8\n4.1\n2\n84\n\n\n5.5\n5.6\n5.6\n4.4\n2\n84\n\n\n6.3\n5.4\n4.8\n4.6\n2\n82\n\n\n4.3\n5.5\n5.5\n4.1\n2\n79\n\n\n4.6\n4.1\n4.3\n3.1\n2\n81\n\n\n3.4\n5.0\n3.4\n3.4\n2\n83\n\n\n6.4\n5.4\n6.6\n4.8\n2\n81\n\n\n5.5\n5.3\n5.3\n3.8\n2\n84\n\n\n4.7\n4.1\n5.0\n3.7\n2\n83\n\n\n4.1\n4.0\n4.1\n4.0\n2\n80\n\n\n\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\nTable 2.11: Correlation Matrix\n\n\n\n\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\nFoam\n1.0000000\n0.0021934\n0.3284925\n0.1931525\n-0.0323318\n0.5122369\n\n\nScent\n0.0021934\n1.0000000\n0.5987719\n0.4995952\n0.2782582\n-0.2516592\n\n\nColor\n0.3284925\n0.5987719\n1.0000000\n0.5241356\n0.4577615\n-0.1938511\n\n\nResidue\n0.1931525\n0.4995952\n0.5241356\n1.0000000\n0.1647598\n-0.4892339\n\n\nRegion\n-0.0323318\n0.2782582\n0.4577615\n0.1647598\n1.0000000\n-0.5071783\n\n\nQuality\n0.5122369\n-0.2516592\n-0.1938511\n-0.4892339\n-0.5071783\n1.0000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.12\n\n\nTable2.11 &lt;- fread(\"./data/Table2.11.csv\")\n\nTable2.11 %&gt;% \n  cor() %&gt;% \n  kbl()\n\n\n\n\n\n\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 2.12: Matrix of scatter plots for the shampoo data\n\n\n\n\n\n\n\n\nlibrary(GGally)\nggpairs(data = Table2.11)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Summary and Presentation</span>"
    ]
  },
  {
    "objectID": "Ch03.html",
    "href": "Ch03.html",
    "title": "3  Random Variables and Probability Distributions",
    "section": "",
    "text": "3.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#introduction",
    "href": "Ch03.html#introduction",
    "title": "3  Random Variables and Probability Distributions",
    "section": "",
    "text": "Introduction\n\n\n\n\nExperiment\nRandom\nRandom experiment\n\nFigure 3.1 illustrates the relationship between a mathematical model and the physical system it represents. While models like Newton’s laws are not perfect abstractions, they are useful for analyzing and approximating system performance. Once validated with measurements, models can help understand, describe, and predict a system’s response to inputs.\n\n\n\n\n\n\nFigure 3.1: Continuous iteration between model and physical system.\n\n\n\nFigure 3.2 illustrates a model where uncontrollable variables (noise) interact with controllable variables to produce system outputs. Due to the noise, identical controllable settings yield varying outputs upon measurement.\n\n\n\n\n\n\nFigure 3.2: Noise variables affect the transformation of inputs to outputs.\n\n\n\nFor measuring current in a copper wire, Ohm’s law may suffice as a model. However, if variations are significant, the model may need to account for them (see Figure 3.3).\n\n\n\n\n\n\nFigure 3.3: A closer examination of the system identifies deviations from the model.\n\n\n\nIn designing a voice communication system, a model is required for call frequency and duration. Even if calls occur and last precisely 5 minutes on average, variations in timing or duration can lead to call blocking, necessitating multiple lines (see Figure 3.4).\n\n\n\n\n\n\nFigure 3.4: Variation causes disruptions in the system.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#random-variables",
    "href": "Ch03.html#random-variables",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.2 Random Variables",
    "text": "3.2 Random Variables\n\n\n\n\n\n\nRandom Variables\n\n\n\n\nIn an experiment, a measurement is usually denoted by a variable such as X.\nIn a random experiment, a variable whose measured value can change (from one replicate of the experiment to another) is referred to as a random variable.\n\n\n\n\n\n\n\n\n\nRandom Variable\n\n\n\nA random variable is a numerical variable whose measured value can change from one replicate of the experiment to another.\n\n\n\n\n\n\n\n\nDiecrete Random Variable\n\n\n\n\nA discrete random variable is a random variable with a finite (or countably infinite) set of real numbers for its range.\nExamples of discrete random variables:\n\nnumber of scratches on a surface, proportion of defective parts among 1000 tested, number of transmitted bits received in error\n\n\n\n\n\n\n\n\n\n\nContinuous Random Variable\n\n\n\n\nA continuous random variable is a random variable with an interval (either finite or infinite) of real numbers for its range.\nExamples of continuous random variables:\n\nelectrical current, length, pressure, temperature, time, voltage, weight",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#probability",
    "href": "Ch03.html#probability",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.3 Probability",
    "text": "3.3 Probability\n\n\n\n\n\n\nProbability\n\n\n\n\nUsed to quantify likelihood or chance\nUsed to represent risk or uncertainty in engineering applications\nCan be interpreted as our degree of belief or relative frequency\nProbability statements describe the likelihood that particular values occur.\nThe likelihood is quantified by assigning a number from the interval [0, 1] to the set of values (or a percentage from 0 to 100%).\nHigher numbers indicate that the set of values is more likely.\nA probability is usually expressed in terms of a random variable.\nFor the part length example, X denotes the part length and the probability statement can be written in either of the following forms P\\left(X\\in\\left[10.8,11.2\\right]\\right)=0.25 or P\\left(10.8\\leq X\\leq11.2\\right)=0.25.\nBoth equations state that the probability that the random variable X assumes a value in \\left[10.8,11.2\\right] is 0.25.\n\n\n\n\n3.3.1 Complement of an Event\n\n\n\n\n\n\nComplement of an Event\n\n\n\nGiven a set E, the complement of E is the set of elements that are not in E. The complement is denoted as E^{\\prime} or E^{c}.\n\n\n\n\n3.3.2 Mutually Exclusive Events\n\n\n\n\n\n\nMutually Exclusive Events\n\n\n\nThe sets E_{1}, E_{2} ,\\ldots,E_{k} are mutually exclusive if the intersection of any pair is empty. That is, each element is in one and only one of the sets E_{1}, E_{2} ,\\ldots,E_{k}.\n\n\n\n\n3.3.3 Probability Properties\n\n\n\n\n\n\nProbability Properties\n\n\n\n\nP\\left(X\\in R\\right)=1, where R is the set of real numbers.\n0\\leq P\\left(X\\in E\\right)\\leq1 for any set E.\nIf E_{1}, E_{2} ,\\ldots,E_{k} are mutually exclusive sets, P\\left(X\\in E_{1}\\cup E_{2}\\cup\\ldots\\cup E_{k}\\right)=P\\left(X\\in E_{1}\\right)+P\\left(X\\in E_{2}\\right)+\\cdots+P\\left(X\\in E_{k}\\right)\n\n\n\n\n\n3.3.4 Events\n\n\n\n\n\n\nEvents\n\n\n\n\nA measured value is not always obtained from an experiment. Sometimes, the result is only classified (into one of several possible categories).\nThese categories are often referred to as events.\nIllustrations\n\nThe current measurement might only be recorded as low, medium, or high; a manufactured electronic component might be classified only as defective or not; and either a message is sent through a network or not.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#continuous-random-variables",
    "href": "Ch03.html#continuous-random-variables",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.4 Continuous Random Variables",
    "text": "3.4 Continuous Random Variables\n\n3.4.1 Probability Density Function\n\n\n\n\n\n\nProbability Density Function\n\n\n\n\nThe probability distribution or simply distribution of a random variable X is a description of the set of the probabilities associated with the possible values for X.\nThe probability density function (or pdf) f(x) of a continuous random variable X is used to determine probabilities as follows: \n  P(a &lt; X &lt; b) = \\int_{a}^{b} f(x)\\,dx\n  \nThe properties of the pdf are\n\nf(x)\\geq 0\n\\int_{-\\infty}^{\\infty}f(x)dx=1\n\nIf X is a continuous random variable, for any x1 and x2, \n  P\\left(x_{1}\\leq X\\leq x_{2}\\right)=P\\left(x_{1}&lt;X\\leq x_{2}\\right)=P\\left(x_{1}\\leq X&lt;x_{2}\\right)=P\\left(x_{1}&lt;X&lt;x_{2}\\right)\n  \n\n\n\n\n\n\n\n\n\nExample\n\n\n\nLet the continuous random variable X denote the distance in micrometers from the start of a track on a magnetic disk until the first flaw. Historical data show that the distribution of X can be modeled by the probability density function (pdf):\n\nf(x) = \\frac{1}{2000} e^{-x/2000}, \\quad x \\geq 0.\n\nFor what proportion of disks is the distance to the first flaw greater than 1000 micrometers? What proportion of parts is between 1000 and 2000 micrometers?\n\n\n\n\n\n\nFigure 3.5: Probability density function\n\n\n\nThe probability that the distance to the first flaw is greater than 1000 micrometers is given by:\n\n\\begin{align*}\nP(X &gt; 1000) &= \\int_{1000}^{\\infty} f(x)\\,dx \\\\\n&= \\int_{1000}^{\\infty} \\frac{1}{2000} e^{-x/2000} \\,dx.\n\\end{align*}\n\nTo evaluate the integral, we use the formula for the integral of an exponential function:\n\\int e^{-ax} \\,dx = \\frac{e^{-ax}}{-a}.\nHere, a = \\frac{1}{2000}, so:\n\n\\begin{align*}\nP(X &gt; 1000) &= \\left[ -e^{-x/2000} \\right]_{1000}^{\\infty} \\\\\n&= \\left( 0 - \\left( -e^{-1000/2000} \\right) \\right) \\\\\n&= e^{-1/2} \\\\\n&\\approx 0.6065.\n\\end{align*}\n\nThus, the proportion of disks where the distance to the first flaw is greater than 1000 micrometers is approximately 0.6065.\nThe probability that the distance to the first flaw is between 1000 and 2000 micrometers is given by:\n\n\\begin{align*}\nP(1000 \\leq X \\leq 2000) &= \\int_{1000}^{2000} f(x)\\,dx \\\\\n&= \\int_{1000}^{2000} \\frac{1}{2000} e^{-x/2000} \\,dx. \\\\\n& = \\left[ -e^{-x/2000} \\right]_{1000}^{2000} \\\\\n&= e^{-1/2} - e^{-1}\\\\\n&= 0.6065 - 0.3679 \\\\\n&= 0.2386.\n\\end{align*}\n\nTherefore, the proportion of parts with a flaw distance between 1000 and 2000 micrometers is approximately 0.2386.\n\nR OutputR Code\n\n\n\n\n[1] 0.6065307\n\n\n[1] 0.2386512\n\n\n\n\n\npdf1 &lt;- function(x) {(1/2000) * exp(-x/2000)}\n\nintegrate(\n            f     = pdf1\n          , lower = 1000\n          , upper = Inf\n          )$value\n\nintegrate(\n            f     = pdf1\n          , lower = 1000\n          , upper = 2000\n          )$value\n\n\n\n\n\n\n\n\n3.4.2 Cumulative Distribution Function\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\nThe cumulative distribution function (or cdf) of a continuous random variable X with probability density function f(x) is \n  F(X) = P(X \\leq x) = \\int_{-\\infty}^{x} f(u)\\,du\n   for -\\infty &lt; x &lt; \\infty.\n\n\nFor a continuous random variable X, the definition can also be F(x)= P(X &lt; x) because P(X = x) = 0.\nThe cumulative distribution function F(x) can be related to the probability density function f(x) and can be used to obtain probabilities as follows.\n\n\\begin{align*}\nP(a&lt;X&lt;b) & = \\int_{a}^{b} f(x)\\,dx \\\\\n& = \\int_{-\\infty}^{b} f(x)\\,dx - \\int_{-\\infty}^{a} f(x)\\,dx \\\\\n& = F(b) - F(a)\n\\end{align*}\n Furthermore, the graph of a cdf has specific properties. Because F(x) provides probabilities, it is always nonnegative. Furthermore, as x increases, F(x) is nondecreasing. Finally, as x tends to infinity, F (x) = P(X \\leq x) tends to 1.\n\n\n\n\n\n\nExample\n\n\n\nLet the continuous random variable X denote the distance in micrometers from the start of a track on a magnetic disk until the first flaw. Historical data show that the distribution of X can be modeled by the probability density function (pdf):\n\nf(x) = \\frac{1}{2000} e^{-x/2000}, \\quad x \\geq 0.\n\nThe cdf is determined from\n\n\\begin{align*}\nF(X) & = P(X \\leq x) = \\int_{0}^{x} \\frac{1}{2000} e^{-u/2000}\\,du = 1- e^{-x/2000}\n\\end{align*}\n\nfor x \\geq 0. It can be checked that \\frac{d}{dx}F(x) = f(x).\n\n\n\n\n\n\nFigure 3.6: Cumulative distribution function\n\n\n\nA graph of F(x) is shown in Figure 3.6. Note that F(x)=0 for x \\leq 0. Also, F(x) increases to 1 as mentioned. The following probabilities should be compared to the results in previous Example.\nDetermine the probability that the distance until the first surface flaw is greater than 1000 micrometers.\n\n\\begin{align*}\nP(X &gt; 1000) &= 1- P(X &lt; 1000) = 1- F(1000) \\\\\n&= 1- \\left(1- e^{-1000/2000} \\right) = e^{-1/2}\\\\\n&\\approx 0.6065.\n\\end{align*}\n\nThus, the proportion of disks where the distance to the first flaw is greater than 1000 micrometers is approximately 0.6065.\nDetermine the probability that the distance is between 1000 and 2000 micrometers.\n\n\\begin{align*}\nP(1000 \\leq X \\leq 2000) &= F(2000) - F(1000) \\\\\n&= \\left(1- e^{-2000/2000} \\right) - \\left(1- e^{-1000/2000} \\right)\\\\\n&= e^{-1/2} - e^{-1}\\\\\n&= 0.6065 - 0.3679 \\\\\n&= 0.2386.\n\\end{align*}\n\nTherefore, the proportion of parts with a flaw distance between 1000 and 2000 micrometers is approximately 0.2386.\n\nR OutputR Code\n\n\n\n\n[1] 0.6065307\n\n\n[1] 0.2386512\n\n\n\n\n\ncdf1 &lt;- function(x) {1 - exp(-x/2000)}\n1 - cdf1(1000)\n\ncdf1(2000) - cdf1(1000)\n\n\n\n\n\n\n\n\n3.4.3 Mean and Variance\n\n\n\n\n\n\nMean and Variance\n\n\n\nSuppose X is a continuous random variable with pdf f(x). The mean or expected value of X, denoted as \\mu or E(X), is\n\n\\mu = E(X) = \\int_{-\\infty}^{\\infty} xf(x)\\,dx.\n\nThe variance of X, denoted as V(X) or \\sigma^{2}, is\n\n\\sigma^{2} = V(X) = \\int_{-\\infty}^{\\infty} (x-\\mu)^{2}f(x)\\,dx = E(X^2) - \\mu^{2}.\n\nThe standard deviation of X is \\sigma.\n\n\n\n\n\n\n\n\nExample\n\n\n\nFor the distance to a flaw in the previous example, the mean of X is\n\n\\begin{align*}\nE(X) & = \\int_{-\\infty}^{\\infty} x f(x)\\,dx = \\int_{0}^{\\infty} x \\frac{e^{-x/2000}}{2000}\\,dx \\\\\n& = \\left[ -xe^{-x/2000} \\right]_{0}^{\\infty} + \\int_{0}^{\\infty}  \\frac{e^{-x/2000}}{2000}\\,dx \\\\\nE(X) & = 0 - \\left[ 2000 e^{-x/2000} \\right]_{0}^{\\infty} = 2000.\n\\end{align*}\n\nThe variance of X is\n\n\\begin{align*}\nV(X) & = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)\\,dx = \\int_{0}^{\\infty} (x-2000)^2 \\frac{e^{-x/2000}}{2000}\\,dx \\\\\nV(X) & = 2000^2 = 4,000,000.\n\\end{align*}\n\n\nR OutputR Code\n\n\n\n\n[1] 2000\n\n\n[1] 4000000\n\n\n\n\n\nMean1 &lt;- function(x) {x*(1/2000) * exp(-x/2000)}\nintegrate(\n            f     = Mean1\n          , lower = 0\n          , upper = Inf\n          )$value\n\noptions(scipen = 999)\n\nVariance1 &lt;- function(x) {(x-2000)^2*(1/2000) * exp(-x/2000)}\nintegrate(\n            f     = Variance1\n          , lower = 0\n          , upper = Inf\n          )$value",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#important-continuous-random-distributions",
    "href": "Ch03.html#important-continuous-random-distributions",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.5 Important Continuous Random Distributions",
    "text": "3.5 Important Continuous Random Distributions\n\n3.5.1 Normal Distribution\n\n\n\n\n\n\nNormal Distribution\n\n\n\nUndoubtedly, the most widely used model for the distribution of a random variable is a normal distribution.\n\nCentral limit theorem\nGaussian distribution\n\n\n\n\n\n\n\nFigure 3.7: Normal probability density functions for selected values of the parameters \\mu and \\sigma^2.\n\n\n\n\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\nA random variable X with probability density function\n\n\\begin{align*}\nf\\left(X\\right)=\\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}\\exp\\left\\{ -\\frac{1}{2}\\left(\\frac{X-\\mu}{\\sigma}\\right)^{2}\\right\\} .\n\\end{align*}\n\nhas a normal distribution (and is called a normal random variable) with parameters \\mu and \\sigma, where -\\infty &lt; \\mu &lt; \\infty and \\sigma &gt; 0. Also\n\nE(X) = \\mu \\quad \\text{and} \\quad V(X) = \\sigma^2\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nAssume that the current measurements in a strip of wire follow a normal distribution with a mean of 10 milliamperes and a variance of 4 \\text{milliamperes}^2. What is the probability that a measurement exceeds 13 milliamperes?\n\nLet X denote the current in milliamperes. The requested probability can be represented as P(X &gt; 13). This probability is shown as the shaded area under the normal probability density function in Figure 3.8. Unfortunately, there is no closed-form expression for the integral of a normal pdf, and probabilities based on the normal distribution are typically found numerically or from a table.\n\n\n\n\n\n\nFigure 3.8: Probability that X &gt; 13 for a normal random variable with \\mu = 10 and \\sigma^2 = 4.\n\n\n\n\n\n\n\n\n\n\n\nNormal Distribution\n\n\n\n\n\\begin{align*}\nP(\\mu - \\sigma &lt; X &lt; \\mu + \\sigma) & = 0.6827\\\\\nP(\\mu - 2\\sigma &lt; X &lt; \\mu + 2\\sigma) & = 0.9545\\\\\nP(\\mu - 3\\sigma &lt; X &lt; \\mu + 3\\sigma) & = 0.9973\\\\\n\\end{align*}\n\n\n\n\n\n\n\nFigure 3.9: Probabilities associated with a normal distribution.\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] 0.6826895\n\n\n[1] 0.9544997\n\n\n[1] 0.9973002\n\n\n\n\n\npnorm(q = 1, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) -\n  pnorm(q = -1, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\npnorm(q = 2, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) -\n  pnorm(q = -2, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\npnorm(q = 3, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) -\n  pnorm(q = -3, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Normal Random Variable\n\n\n\nA normal random variable with \\mu = 0 and \\sigma^2 = 1 is called a standard normal random variable. A standard normal random variable is denoted as Z.\n\n\n\n\n\n\n\n\nStandard Normal Cumulative Distribution Function\n\n\n\nThe function \n\\Phi(z) = P(Z \\leq z)\n\nis used to denote the cumulative distribution function of a standard normal random variable. A table (or computer software) is required because the probability cannot be calculated in general by elementary methods.\n\n\n\n\n\n\n\n\nFigure 3.10: Standard normal probability density function.\n\n\n\n\n\n\n\n\n\n\nFigure 3.11: Cumulative Standard Normal Distribution\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe following calculations are shown pictorially in Figure 3.12.\n\nP(Z &gt; 1.26) = 1 - P(Z \\leq 1.26) = 1 - 0.89616 = 0.10384\nP(Z &lt; - 0.86) = 0.19490\nP(Z &gt; - 1.37) = P(Z &lt; 1.37) = 0.91465\nP(-1.25 &lt; Z &lt; 0.37) = P(Z &lt; 0.37) - P(Z &lt; -1.25) = 0.64431 - 0.10565 = 0.53866\nP(Z \\leq -4.6) \\approx 0\nFind the value z such that P(Z &gt; z) = 0.05. This probability equation can be written as P(Z \\leq z) = 0.95. Now, Table I (Figure 3.11) is used in reverse. We search through the probabilities to find the value that corresponds to 0.95. The solution is illustrated in Figure 3.12. We do not find 0.95 exactly;the nearest value is 0.95053, corresponding to z = 1.65.\nFind the value of z such that P(-z &lt; Z &lt; z) = 0.99. Because of the symmetry of the normal distribution, if the area of the shaded region in Fig. Figure 3.12 is to equal 0.99, the area in each tail of the distribution must equal 0.005. Therefore, the value for z corresponds to a probability of 0.995 in Table I. The nearest probability in Table I (Figure 3.11) is 0.99506, when z = 2.58.\n\n\n\n\n\n\n\nFigure 3.12: Graphical displays for Example.\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] 0.1038347\n\n\n[1] 0.1038347\n\n\n[1] 0.1948945\n\n\n[1] 0.9146565\n\n\n[1] 0.9146565\n\n\n[1] 0.538659\n\n\n[1] 0.000002112455\n\n\n[1] 1.644854\n\n\n[1] 2.575829\n\n\n\n\n\n1 - pnorm(q = 1.26, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\npnorm(q = 1.26, mean = 0, sd = 1, lower.tail = FALSE, log.p = FALSE)\n\npnorm(q = -0.86, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\n\n1 - pnorm(q = -1.37, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\npnorm(q = -1.37, mean = 0, sd = 1, lower.tail = FALSE, log.p = FALSE)\n\n\npnorm(q = 0.37, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) -\n  pnorm(q = -1.25, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\n\npnorm(q = -4.6, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\nqnorm(p = 0.05, mean = 0, sd = 1, lower.tail = FALSE, log.p = FALSE)\nqnorm(p = 0.995, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\nStandard Normal Random Variable\n\n\n\nIf X is a normal random variable with E(X) = \\mu and V(X) = \\sigma^2, the random variable  Z = \\frac{X-\\mu}{\\sigma}\nis a normal random variable with E(Z ) = 0 and V(Z ) = 1. That is, Z is a standard normal random variable.\n\n\n\n\n\n\n\n\nStandardizing\n\n\n\nSuppose X is a normal random variable with mean \\mu and variance \\sigma^2. Then\n P\\left(X \\leq x\\right) = P\\left(\\frac{X-\\mu}{\\sigma} \\leq \\frac{x-\\mu}{\\sigma}\\right)  = P\\left(Z \\leq z\\right)  where Z is a standard normal random variable, and and z = \\frac{x-\\mu}{\\sigma} is the z-value obtained by standardizing x. The probability is obtained by entering Appendix A Table I (Figure 3.11) with z = \\frac{x-\\mu}{\\sigma}.\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe diameter of a shaft in a storage drive is normally distributed with mean 0.2508 inch and standard deviation 0.0005 inch. The specifications on the shaft are 0.2500 \\pm 0.0015 inch. What proportion of shafts conforms to specifications?\nLet X denote the shaft diameter in inches. The requested probability is shown in Figure 3.13 and \n\\begin{align*}\nP\\left(0.2485 &lt; X &lt; 0.2515\\right) & = P\\left(\\frac{0.2485-0.2508}{0.0005} \\leq \\frac{0.2515-0.2508}{0.0005}\\right)\\\\\n&= P\\left(-4.6&lt; Z &lt; 1.4\\right) = P\\left(Z &lt; 1.4\\right) -P\\left(Z &lt; -4.6\\right)\\\\\n&= 0.91924-0.00000=0.91924\n\\end{align*}\n\n\n\n\n\n\n\nFigure 3.13: Distribution\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] 0.9192412\n\n\n\n\n\npnorm(q = 0.2515, mean = 0.2508, sd = 0.0005, lower.tail = TRUE, log.p = FALSE) -\n  pnorm(q = 0.2485, mean = 0.2508, sd = 0.0005, lower.tail = TRUE, log.p = FALSE)\n\n\n\n\n\n\n\n\n3.5.2 Lognormal Distribution\n\n\n\n\n\n\nLognormal Distribution\n\n\n\nLet W have a normal distribution with mean \\theta and variance \\omega^{2}. Then X=\\exp\\left(W\\right) is a lognormal random variable with probability density function\n\nf\\left(x\\right)=\\frac{1}{x\\sqrt{2\\pi\\omega^{2}}}\\exp\\left\\{ -\\frac{1}{2}\\left(\\frac{\\ln\\left(x\\right)-\\theta}{\\omega}\\right)^{2}\\right\\} \\quad\\text{for}\\quad0&lt;x&lt;\\infty\n\nThe mean and variance of X are \nE\\left(X\\right)=\\exp\\left(\\theta+\\frac{1}{2}\\omega^{2}\\right)\n and \nV\\left(X\\right)=\\exp\\left(2\\theta+\\omega^{2}\\right)\\left\\{ \\exp\\left(\\omega^{2}\\right)-1\\right\\}\n respectively.\n\n\n\n\n\n\nFigure 3.14: Lognormal probability density functions with \\theta = 0 for selected values of \\omega^{2}.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe lifetime of a semiconductor laser has a lognormal distribution with \\theta = 10 and \\omega = 1.5 hours. What is the probability the lifetime exceeds 10,000 hours? What lifetime is exceeded by 99% of lasers? Determine the mean and variance of lifetime.\nThe random variable X is the lifetime of a semiconductor laser. From the cumulative distribution function for X\n\n\\begin{align*}\nP\\left(X &gt; 10,000\\right) & = 1- P\\left(X &lt; 10,000\\right)\\\\\n& = 1- P\\left(\\exp(W) &lt; 10,000\\right)\\\\\n& = 1- P\\left(W &lt; \\ln(10,000) \\right)\\\\\n& = 1- P\\left(\\frac{W-\\theta}{\\omega} &lt; \\frac{\\ln(10,000)-10}{1.5} \\right)\\\\\nP\\left(X &gt; 10,000\\right)&= 1- \\Phi\\left(-0.52 \\right) = 1 - 0.30 = 0.70\n\\end{align*}\n Now the question is to determine x such that P(X&gt;x) = 0.99. Therefore,\n\n\\begin{align*}\nP\\left(X &gt; x\\right) & = 1- P\\left(X &lt; x\\right)\\\\\n& = 1- P\\left(\\exp(W) &lt; x\\right)\\\\\n& = 1- P\\left(W &lt; \\ln(x) \\right)\\\\\n& = 1- P\\left(\\frac{W-\\theta}{\\omega} &lt; \\frac{\\ln(x)-10}{1.5} \\right)\\\\\n&= 0.99\n\\end{align*}\n From Figure 3.11, 1 - \\Phi(z) = 0.99 when z = -2.33. Therefore, \\frac{\\ln(x)-10}{1.5} = -2.3263 and x = \\exp(6.5105) \\approx 672.5\\, \\text{hours}.\nThe mean of lifetime is \n\\begin{align*}\nE\\left(X\\right)&=\\exp\\left(\\theta+\\frac{1}{2}\\omega^{2}\\right)\\\\\nE\\left(X\\right)& = \\exp\\left(10+\\frac{1}{2}\\times1.5^{2}\\right) = 67,846.3\n\\end{align*}\n The variance of lifetime is \n\\begin{align*}\nV\\left(X\\right)&=\\exp\\left(2\\theta+\\omega^{2}\\right)\\left\\{ \\exp\\left(\\omega^{2}\\right)-1\\right\\}\\\\\n& = \\exp\\left(2\\times10+1.5^{2}\\right)\\left\\{ \\exp\\left(1.5^{2}\\right)-1\\right\\}\\\\\nV\\left(X\\right)& = 39,070,059,886.6\n\\end{align*}\n\n\nR OutputR Code\n\n\n\n\n[1] 0.7007086\n\n\n[1] 672.1478\n\n\n[1] 672.1478\n\n\n[1] 67846.29\n\n\n[1] 39070059887\n\n\n\n\n\n1 - plnorm(q = 10000, meanlog = 10, sdlog = 1.5, lower.tail = TRUE, log.p = FALSE)\n\nqlnorm(p = 0.99, meanlog = 10, sdlog = 1.5, lower.tail = FALSE, log.p = FALSE)\nqlnorm(p = 0.01, meanlog = 10, sdlog = 1.5, lower.tail = TRUE, log.p = FALSE)\n\nexp(10 + (1.5^2) / 2)\n\nexp(2*10 + 1.5^2)*(exp(1.5^2)-1)\n\n\n\n\n\n\n\n\n3.5.3 Gamma Distribution\n\n\n\n\n\n\nGamma Distribution\n\n\n\nThe random variable X with probability density function \nf\\left(x\\right)=\\frac{\\lambda^{r}x^{r-1}\\exp\\left(-\\lambda x\\right)}{\\Gamma\\left(r\\right)}\\quad\\text{for}\\quad x&gt;0\n\nis a gamma random variable with shape parameter r&gt;0 and rate parameter \\lambda&gt;0. \\Gamma\\left(r\\right) is the gamma function defined as: \\Gamma\\left(r\\right)=\\int_{0}^{\\infty} x^{r-1} \\exp(-x)\\,dx, for r&gt;0. The mean and variance of X are \nE\\left(X\\right)=\\frac{r}{\\lambda}\n and \nV\\left(X\\right)=\\frac{r}{\\lambda^{2}}\n respectively.\n\n\n\n\n\n\nFigure 3.15: Gamma probability density functions for selected values of \\lambda and r.\n\n\n\n\n\n\n\n3.5.4 Weibull Distribution\n\n\n\n\n\n\nWeibull Distribution\n\n\n\nThe random variable X with probability density function \nf\\left(x\\right)=\\frac{\\beta}{\\delta}\\left(\\frac{x}{\\delta}\\right)^{\\beta-1}\\exp\\left\\{ -\\left(\\frac{x}{\\delta}\\right)^{\\beta}\\right\\} \\quad\\text{for}\\quad x&gt;0\n\nis a Weibull random variable with scale parameter \\delta&gt;0 and shape parameter \\beta&gt;0.\nIf X has a Weibull distribution with parameters \\delta&gt;0 and \\beta&gt;0, the cumulative distribution function of X is\n\nF\\left(x\\right)=1 - \\exp\\left\\{ -\\left(\\frac{x}{\\delta}\\right)^{\\beta}\\right\\}\n\nThe mean and variance of X are \nE\\left(X\\right)=\\delta\\Gamma\\left(1+\\frac{1}{\\beta}\\right)\n and \nV\\left(X\\right)=\\delta^{2}\\Gamma\\left(1+\\frac{2}{\\beta}\\right)-\\delta^{2}\\left\\{ \\Gamma\\left(1+\\frac{1}{\\beta}\\right)\\right\\} ^{2}\n respectively.\n\n\n\n\n\n\nFigure 3.16: Weibull probability density functions for selected values of \\delta and \\beta.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe time to failure (in hours) of a bearing in a mechanical shaft is satisfactorily modeled as a Weibull random variable with $= $ and \\delta=5000 hours. Determine the mean time until failure. Determine the probability that a bearing lasts at least 6000 hours.\nThe mean time until failure is \n\\begin{align*}\nE\\left(X\\right)&=\\delta\\Gamma\\left(1+\\frac{1}{\\beta}\\right)\\\\\n&=5000\\times\\Gamma\\left(1+2\\right)\\\\\n&= 5000\\times2! = 10,000 \\, \\text{hours}\n\\end{align*}\n The probability that a bearing lasts at least 6000 hours is\n\n\\begin{align*}\nP\\left(X &gt; 6000\\right)&= 1- F\\left(6000\\right)\\\\\n&=\\exp\\left\\{ -\\left(\\frac{6000}{5000}\\right)^{1.5}\\right\\}\\\\\n&= 0.334\\\\\n\\end{align*}\n Consequently, only 33.4% of all bearings last at least 6000 hours.\n\nR OutputR Code\n\n\n\n\n[1] 10000\n\n\n[1] 0.3343907\n\n\n\n\n\n5000*gamma(1+1/0.5)\n\n1 - pweibull(q = 6000, shape = 0.5, scale = 5000, lower.tail = TRUE, log.p = FALSE)\n\n\n\n\n\n\n\n\n3.5.5 Beta Distribution\n\n\n\n\n\n\nBeta Distribution\n\n\n\nThe random variable X with probability density function \nf\\left(x\\right)=\\frac{\\Gamma\\left(\\alpha+\\beta\\right)}{\\Gamma\\left(\\alpha\\right)\\Gamma\\left(\\beta\\right)}x^{\\alpha-1}\\left(1-x\\right)^{\\beta-1}\\quad\\text{for}\\quad0\\leq x\\leq1\n\nis a beta random variable with parameters \\alpha&gt;0 and \\beta&gt;0. The mean and variance of X are \nE\\left(X\\right)=\\frac{\\alpha}{\\alpha+\\beta}\n\nand \nV\\left(X\\right)=\\frac{\\alpha\\beta}{\\left(\\alpha+\\beta\\right)^{2}\\left(\\alpha+\\beta+1\\right)}\n respectively.\n\n\n\n\n\n\nFigure 3.17: Beta probability density functions for selected values of the parameters \\alpha and \\beta.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nConsider the completion time of a large commercial development. The proportion of the maximum allowed time to complete a task is modeled as a beta random variable with \\alpha=2.5 and \\beta=1. What is the probability that the proportion of the maximum time exceeds 0.7? Calculate the mean and variance of this random variable.\nSuppose X denotes the proportion of the maximum time required to complete the task. The probability is\n\n\\begin{align*}\nP\\left(X&gt;0.7\\right)&=\\int_{0.7}^{1} \\frac{\\Gamma\\left(\\alpha+\\beta\\right)}{\\Gamma\\left(\\alpha\\right)\\Gamma\\left(\\beta\\right)}x^{\\alpha-1}\\left(1-x\\right)^{\\beta-1}\\,dx\\\\\n& = \\int_{0.7}^{1} \\frac{\\Gamma\\left(2.5+1\\right)}{\\Gamma\\left(2.5\\right)\\Gamma\\left(1\\right)}x^{2.5-1}\\left(1-x\\right)^{1-1}\\,dx\\\\\n& = \\int_{0.7}^{1} \\frac{\\Gamma\\left(3.5\\right)}{\\Gamma\\left(2.5\\right)\\Gamma\\left(1\\right)}x^{1.5}\\,dx\\\\\n& = \\frac{2.5(1.5)(0.5)\\sqrt{\\pi}}{(1.5)(0.5)\\sqrt{\\pi}} \\frac{1}{2.5}  x^{2.5} \\bigg|_{0.7}^{1}\\\\\nP\\left(X&gt;0.7\\right)&= 1 - 0.7^{2.5} = 0.59\n\\end{align*}\n\nThe mean is\n\n\\begin{align*}\nE\\left(X\\right)&=\\frac{\\alpha}{\\alpha+\\beta}\\\\\n& = \\frac{2.5}{2.5+1} = 0.71\n\\end{align*}\n\nand \n\\begin{align*}\nV\\left(X\\right)&=\\frac{\\alpha\\beta}{\\left(\\alpha+\\beta\\right)^{2}\\left(\\alpha+\\beta+1\\right)}\\\\\n&=\\frac{2.5\\times1}{\\left(2.5+1\\right)^{2}\\left(2.5+1+1\\right)}\\\\\n& = 0.045\n\\end{align*}\n\n\nR OutputR Code\n\n\n\n\n[1] 0.5900366\n\n\n[1] 0.7142857\n\n\n[1] 0.04535147\n\n\n\n\n\n1- pbeta(q = 0.7, shape1 = 2.5, shape2 = 1, ncp = 0, lower.tail = TRUE, log.p = FALSE)\n\n2.5/(2.5 + 1)\n\n(2.5 * 1)/((2.5 + 1)^2 * (2.5 + 1 + 1))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#probability-plots",
    "href": "Ch03.html#probability-plots",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.6 Probability Plots",
    "text": "3.6 Probability Plots\n\n3.6.1 Normal Probability Plots\n\n\n3.6.2 Other Probability Plots",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#discrete-random-variables",
    "href": "Ch03.html#discrete-random-variables",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.7 Discrete Random Variables",
    "text": "3.7 Discrete Random Variables\n\n\n\n\n\n\nDiscrete Random Variables\n\n\n\n\nOnly measurements at discrete points are possible\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe analysis of the surface of a semiconductor wafer records the number of particles of contamination that exceed a certain size. Define the random variable X to equal the number of particles of contamination. The possible values of X are integers from 0 up to some large value that represents the maximum number of these particles that can be found on one of the wafers. If this maximum number is very large, it might be convenient to assume that any integer from zero to is possible.\n\n\n\n3.7.1 Probability Mass Function\n\n\n\n\n\n\nProbability Mass Function\n\n\n\nFor a discrete random variable X with possible values x_{1},x_{2},\\ldots,x_{n}, the probability mass function (or PMF) is\n\nf(x_{i})=P(X=x_{i})\n\nBecause f(xi) is defined as a probability,\n\nf(x_{i}) \\geq 0 for all x_{i}\n\\sum_{i=1}^{n}f(x_{i})=1.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nThere is a chance that a bit transmitted through a digital transmission channel is received in error. Let X equal the number of bits in error in the next 4 bits transmitted. The possible values for X are {0, 1, 2, 3, 4}. Based on a model for the errors that is presented in the following section, probabilities for these values will be determined. Suppose that the probabilities are P(X = 0) = 0.6561, P(X = 1) = 0.2916, P(X = 2) = 0.0486, P(X = 3) = 0.0036, P(X = 4) = 0.0001.\nThe probability distribution of X is specified by the possible values along with the probability of each. A graphical description of the probability distribution of X is shown in Figure 3.18.\n\n\n\n\n\n\nFigure 3.18: Probability distribution for X.\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(fastverse)\nlibrary(tidyverse)\nlibrary(scales)\n\ndf3 &lt;-\n  data.table(\n    X  = c(0, 1, 2, 3, 4)\n  , Px = c(0.6561, 0.2916, 0.0486, 0.0036, 0.0001) \n  )\n\nggplot(data = df3, mapping = aes(x = X, y = Px)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", alpha = 0.6) +\n  geom_point(color = \"red\", size = 3) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), breaks = pretty_breaks(n = 8)) +\n  labs(\n    title = \"Probability Mass Function (PMF)\"\n  ,  x = \"X\"\n  , y = \"f(X)\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n3.7.2 Cumulative Distribution Function\n\n\n\n\n\n\nCumulative Distribution Function\n\n\n\nThe cumulative distribution function of a discrete random variable X is\n\nF(x)=P(X\\leq x)=\\sum_{x_{i}\\leq x}f(x_{i})\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nIn the previous example, the probability mass function of X is\nP(X = 0) = 0.6561, P(X = 1) = 0.2916, P(X = 2) = 0.0486, P(X = 3) = 0.0036, P(X = 4) = 0.0001.\nTherefore, F(0) = 0.6561, F(1) = 0.6561 + 0.2916 = 0.9477, F(2) = 0.6561 + 0.2916 + 0.0486 = 0.9963, F(3) = 0.9999, F(4) = 1\nEven if the random variable can assume only integer values, the cdf is defined at noninteger values. For example, \nF(1.5) = P(X \\le 1.5) = P(X \\le 1) = 0.9477\n\nThe graph of F(x) is shown in Figure 3.19. Note that the graph has discontinuities (jumps) at the discrete values for X. It is a piecewise continuous function. The size of the jump at a point x equals the probability at x. For example, consider x = 1. Here F(1) = 0.9477, but for 0 \\le x &lt; 1, F(x) = 0.6561. The change is P(X = 1) = 0.2916.\n\n\n\n\n\n\nFigure 3.19: Cumulative distribution function for x.\n\n\n\n\nR OutputR Code\n\n\n\n\n       X     Px     Fx\n   &lt;num&gt;  &lt;num&gt;  &lt;num&gt;\n1:     0 0.6561 0.6561\n2:     1 0.2916 0.9477\n3:     2 0.0486 0.9963\n4:     3 0.0036 0.9999\n5:     4 0.0001 1.0000\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(fastverse)\nlibrary(tidyverse)\nlibrary(scales)\n\ndf3 &lt;-\n  data.table(\n    X  = c(0, 1, 2, 3, 4)\n  , Px = c(0.6561, 0.2916, 0.0486, 0.0036, 0.0001) \n  )\n\ndf4 &lt;- \n  df3 %&gt;% \n  fmutate(Fx = cumsum(Px))\n\ndf4\nggplot(data = df4, mapping = aes(x = X, y = Fx)) +\n  geom_step(direction = \"hv\", color = \"blue\", linewidth = 1) +\n  geom_point(color = \"red\", size = 3) +\n  labs(\n    title = \"Cumulative Distribution Function (CDF)\"\n  , x     = \"X\"\n  , y     = \"F(x)\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n3.7.3 Mean and Variance\n\n\n\n\n\n\nMean and Variance\n\n\n\nLet the possible values of the random variable X be denoted x_{1},x_{2},\\ldots,x_{n}. The pmf of X is f(x), so f(x_{i})=P(X=x_{i}).\nThe mean or expected value of the discrete random variable X, denoted as \\mu or E(X), is\n\n\\mu=E(X)=\\sum_{i=1}^{n}x_{i}f(x_{i})\n\nThe variance of X, denoted as \\sigma^2 or V(X), is\n\n\\begin{align*}\n\\sigma^{2} &    =V\\left(X\\right)=E\\left(X-\\mu\\right)^{2}\\\\\n\\sigma^{2} &    =\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}f\\left(x_{i}\\right) = \\sum_{i=1}^{n}x_{i}^{2}f\\left(x_{i}\\right) - \\mu^2\n\\end{align*}\n\nThe standard deviation of X is \\sigma.\n\n\n\n\n\n\n\n\nExample\n\n\n\nFor the number of bits in error in the previous example,\n\n\\begin{align*}\n\\mu &   =E\\left(X\\right)=0f(0)+1f(1)+2f(2)+3f(3)+4f(4)\\\\\n&  =0(0.6561) + 1(0.2916) + 2(0.0486) + 3(0.0036) + 4(0.0001)\\\\\n& = 0.4\n\\end{align*}\n\nAlthough X never assumes the value 0.4, the weighted average of the possible values is 0.4.\n\n\n\nx\nx-0.4\n(x-0.4)^2\nf(x)\n(x-0.4)^2f(x)\n\n\n\n\n0\n-0.4\n0.16\n0.6561\n0.104976\n\n\n1\n0.6\n0.36\n0.2916\n0.104976\n\n\n2\n1.6\n2.56\n0.0486\n0.124416\n\n\n3\n2.6\n6.76\n0.0036\n0.024336\n\n\n4\n3.6\n12.96\n0.0001\n0.001296\n\n\n\n\nV(X) = \\sigma^2 = \\sum_{i=1}^{n}\\left(x_{i}-0.4\\right)^{2}f\\left(x_{i}\\right) = 0.36\n\n\nR OutputR Code\n\n\n\n\n[1] 0.4\n\n\n[1] 0.36\n\n\n\n\n\nlibrary(fastverse)\nlibrary(tidyverse)\nlibrary(scales)\n\ndf3 &lt;-\n  data.table(\n    X  = c(0, 1, 2, 3, 4)\n  , Px = c(0.6561, 0.2916, 0.0486, 0.0036, 0.0001) \n  )\n\ndf3 %&gt;% \n  fmutate(XPx = X*Px) %&gt;% \n  pull(XPx) %&gt;% \n  fsum(x = .)\n\ndf3 %&gt;% \n  fmutate(V1 = (X-0.4)^2*Px) %&gt;% \n  pull(V1) %&gt;% \n  fsum(x = .)\n\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nTwo new product designs are to be compared on the basis of revenue potential. Marketing feels that the revenue from design A can be predicted quite accurately to be $3 million. The revenue potential of design B is more difficult to assess. Marketing concludes that there is a probability of 0.3 that the revenue from design B will be $7 million, but there is a 0.7 probability that the revenue will be only $2 million. Which design would you choose?\nLet X denote the revenue from design A. Because there is no uncertainty in the revenue from design A, we can model the distribution of the random variable X as $3 million with probability one. Therefore, E(X ) = \\$3 million.\nLet Y denote the revenue from design B. The expected value of Y in millions of dollars is \nE(Y ) = \\$7(0.3) + \\$2(0.7) = \\$3.5\n\nBecause E(Y) exceeds E(X), we might choose design B. However, the variability of the result from design B is larger. That is, \n\\begin{align*}\n\\sigma^2 &= (7-3.5)^{2}(0.3) + (2 - 3.5)^{2}(0.7)\\\\\n\\sigma^2 & = 5.25\\, \\text{millions of dollars squared}\n\\end{align*}\n\nand \n\\sigma = \\sqrt{5.25} = 2.29\\, \\text{millions of dollars}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#binomial-distribution",
    "href": "Ch03.html#binomial-distribution",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.8 Binomial Distribution",
    "text": "3.8 Binomial Distribution\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\nA trial with only two possible outcomes is used so frequently as a building block of a random experiment that it is called a Bernoulli trial.\nIt is usually assumed that the trials that constitute the random experiment are independent. This implies that the outcome from one trial has no effect on the outcome to be obtained from any other trial.\nFurthermore, it is often reasonable to assume that the probability of a success on each trial is constant.\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\n\n\n\nConsider the following random experiments and random variables.\n\nFlip a coin 10 times. Let X = the number of heads obtained.\nOf all bits transmitted through a digital transmission channel, 10% are received in error. Let X = the number of bits in error in the next 4 bits transmitted.\n\n\nDo they meet the following criteria:\n\nDoes the experiment consist of Bernoulli trials?\nAre the trials that constitute the random experiment are independent?\nIs probability of a success on each trial is constant?\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\n\n\n\nA random experiment consisting of n repeated trials such that 1. the trials are independent, 2. each trial results in only two possible outcomes, labeled as success and failure, and 3. the probability of a success on each trial, denoted as p, remains constant\nis called a binomial experiment.\nThe random variable X that equals the number of trials resulting in a success has a binomial distribution with parameters p and p where 0\\leq p\\leq1 and n=\\left\\{ 1,2,3,\\ldots,\\right\\}.\nThe PMF of X is \nf\\left(x\\right)=\\binom{n}{x}p^{x}\\left(1-p\\right)^{n-x}\\,\\text{for}\\,x=0,1,\\ldots,n\n where {n \\choose x}=C_{x}^{n}=\\frac{n!}{x!\\left(n-x\\right)!}.\nIf X is a binomial random variable with parameters n and p, then\n\n\\mu=E\\left(X\\right)=np\n and \n\\sigma^{2}=V\\left(X\\right)=np\\left(1-p\\right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 3.20: Binomial distribution for selected values of n and p.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nEach sample of water has a 10% chance of containing high levels of organic solids. Assume that the samples are independent with regard to the presence of the solids. Determine the probability that in the next 18 samples, exactly 2 contain high solids. Determine the probability that at least four samples contain high solids. Determine, the probability that 3 \\le X \\le 7. Determine, the mean and variance of X.\n\nLet X = the number of samples that contain high solids in the next 18 samples analyzed. Then X is a binomial random variable with p = 0.1 and n = 18. Therefore,\n\nP\\left(X = 2\\right)=\\binom{18}{2}(0.1)^{2}\\left(1-0.1\\right)^{18-2} = 0.284\n\n\n\\begin{align*}\nP\\left(X \\ge 4 \\right) & =\\sum_{x=4}^{18}\\binom{18}{x}(0.1)^{x}\\left(1-0.1\\right)^{18-x} \\\\\nP\\left(X \\ge 4 \\right) & = 1- P\\left(X &lt; 4 \\right)\\\\\n& = 1 - \\sum_{x=0}^{3}\\binom{18}{x}(0.1)^{x}\\left(1-0.1\\right)^{18-x} \\\\\n& = 1 - [0.150 + 0.300 + 0.284 + 0.168]\\\\\nP\\left(X \\ge 4 \\right) = 0.098\n\\end{align*}\n\nAnd\n\n\\begin{align*}\n\\mu & = E\\left(X\\right)=np\\\\\n\\mu & = 4\\times0.1=0.4\n\\end{align*}\n\n\n\\begin{align*}\n\\sigma^{2}&=V\\left(X\\right)=np\\left(1-p\\right)\\\\\n\\sigma^{2}&= 4\\times0.1\\times0.9=0.36\n\\end{align*}\n\n\nR OutputR Code\n\n\n\n\n[1] 0.2835121\n\n\n[1] 153\n\n\n[1] 0.09819684\n\n\n[1] 0.09819684\n\n\n[1] 0.09819684\n\n\n[1] 0.09819684\n\n\n[1] 1.8\n\n\n[1] 1.62\n\n\n\n\n\ndbinom(x = 2, size = 18, prob = 0.1)\nchoose(n = 18, k = 2)\n\nsum(dbinom(x = 4:18, size = 18, prob = 0.1))\n1 - sum(dbinom(x = 0:3, size = 18, prob = 0.1))\n1 - pbinom(q = 3, size = 18, prob = 0.1)\npbinom(q = 3, size = 18, prob = 0.1, lower.tail = FALSE)\n\n18*0.1\n18*0.1*(1-0.1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#poisson-process",
    "href": "Ch03.html#poisson-process",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.9 Poisson Process",
    "text": "3.9 Poisson Process\n\n\n\n\n\n\nPoisson Process\n\n\n\nConsider e-mail messages that arrive at a mail server on a computer network. This is an example of events (such as message arrivals) that occur randomly in an interval (such as time). The number of events over an interval (such as the number of messages that arrive in 1 hour) is a discrete random variable that is often modeled by a Poisson distribution. The length of the interval between events (such as the time between messages) is often modeled by an exponential distribution. These distributions are related; they provide probabilities for different random variables in the same random experiment. Figure 3.21 provides a graphical summary.\n\n\n\n\n\n\nFigure 3.21: In a Poission process, events occur at random in an interval.\n\n\n\n\n\n\n3.9.1 Poisson Distribution\n\n\n\n\n\n\nPoisson Process\n\n\n\nIn general, consider an interval T of real numbers partitioned into subintervals of small length \\Delta t and assume that as \\Delta t tends to zero,\n\nthe probability of more than one event in a subinterval tends to zero\nthe probability of one event in a subinterval tends to \\lambda \\Delta t/T\nthe event in each subinterval is independent of other subintervals.\n\nA random experiment with these properties is called a Poisson process.\n\n\n\n\n\n\n\n\nPoisson Distribution\n\n\n\nThe random variable X that equals the number of events in a Poisson process is a Poisson random variable with parameter \\lambda&gt;0, and the probability mass function of X is\n\nf\\left(x\\right)=\\frac{\\lambda^{x}\\exp\\left(-\\lambda\\right)}{x!}\\quad\\quad x=0,1,2,\\ldots\n The mean and variance of X are \nE\\left(X\\right)=\\lambda\n\nand \nV\\left(X\\right)=\\lambda\n respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure 3.22: Poisson distribution for selected values of the parameter \\lambda.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nFor the case of the thin copper wire, suppose that the number of flaws follows a Poisson distribution with a mean of 2.3 flaws per millimeter. Determine the probability of exactly 2 flaws in 1 millimeter of wire.\nLet X denote the number of flaws in 1 millimeter of wire. Then X has a Poisson distribution and E(X ) = 2.3 flaws and\n\nP(X = 2) = \\frac{\\exp(-2.3)\\times 2.3^2}{2!} = 0.265\n\n\nDetermine the probability of 10 flaws in 5 millimeters of wire.\nLet X denote the number of flaws in 5 millimeter of wire. Then X has a Poisson distribution with \nE(X) = 5 \\,\\text{mm} \\times 2.3 \\, \\text{flaws/mm} = 11.5\\, \\text{flaws}\n\nTherefore,\n\nP(X = 10) = \\frac{\\exp(-11.5)\\times 11.5^10}{10!} = 0.113\n\n\nR OutputR Code\n\n\n\n\n[1] 0.2651846\n\n\n[1] 0.1129351\n\n\n\n\n\ndpois(x = 2, lambda = 2.3)\ndpois(x = 10, lambda = 11.5)\n\n\n\n\n\n\n\n\n3.9.2 Exponential Distribution\n\n\n\n\n\n\nExponential Distribution\n\n\n\n\nThe discussion of the Poisson distribution defined a random variable to be the number of flaws along a length of copper wire. The distance between flaws is another random variable that is often of interest.\nLet the random variable X denote the length from any starting point on the wire until a flaw is detected.\nAs you might expect, the distribution of X can be obtained from knowledge of the distribution of the number of flaws. The key to the relationship is the following concept: the distance to the first flaw exceeds 3 millimeters if and only if there are no flaws within a length of 3 millimeters—simple but sufficient for an analysis of the distribution of X.\n\n\n\n\n\n\n\n\n\nExponential Distribution\n\n\n\nThe random variable X that equals the distance between successive events of a Poisson process with mean \\lambda&gt;0 has an exponential distribution with parameter \\lambda.\nThe probability distribution function of X is given by \nf\\left(x\\right)=\\lambda\\exp\\left(-\\lambda x\\right)\\quad\\quad\\text{for}\\quad0\\leq x&lt;\\infty\n\nThe cumulative distribution function of X is given by \nF\\left(x\\right)=1-\\exp\\left(-\\lambda x\\right)\n\nThe mean and variance of X are \nE\\left(X\\right)=\\frac{1}{\\lambda}\n\nand \nV\\left(X\\right)=\\frac{1}{\\lambda^{2}}\n respectively.\n\n\n\n\n\n\n\nFigure 3.23: Probability density function of an exponential random variable for selected values of \\lambda.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nIn a large corporate computer network, user log-ons to the system can be modeled as a Poisson process with a mean of 25 log-ons per hour. What is the probability that there are no log-ons in an interval of 6 minutes?\nLet X denote the time in hours from the start of the interval until the first log-on. Then X has an exponential distribution with \\lambda = 25 log-ons per hour. We are interested in the probability that X exceeds 6 minutes. Because is given in log-ons per hour, we express all time units in hours; that is, 6 minutes = 0.1 hour. Therefore,\n\n\\begin{align*}\nP\\left(X&gt;0.1\\right) & = \\int_{0.1}^{\\infty} 25\\exp(-25x)\\,dx \\\\\nP\\left(X&gt;0.1\\right) & = \\exp\\left(-25(0.1)\\right) = 0.082\n\\end{align*}\n\n\nR OutputR Code\n\n\n\n\n[1] 0.082085\n\n\n[1] 0.082085\n\n\n\n\n\npexp(q = 0.1, rate = 25, lower.tail = FALSE)\n1 - pexp(q = 0.1, rate = 25) \n\n\n\n\n\n\n\n\n\n\n\n\nExponential Distribution\n\n\n\n\nThe exponential distribution is often used in reliability studies as the model for the time until failure of a device.\nFor example, the lifetime of a semiconductor chip might be modeled as an exponential random variable with a mean of 40,000 hours. The lack of memory property of the exponential distribution implies that the device does not wear out. The lifetime of a device with failures caused by random shocks might be appropriately modeled as an exponential random variable.\nHowever, the lifetime of a device that suffers slow mechanical wear, such as bearing wear, is better modeled by a distribution that does not lack memory, such as the Weibull distribution (with \\beta \\ne 1).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#normal-approximation-to-the-binomial-and-poisson-distributions",
    "href": "Ch03.html#normal-approximation-to-the-binomial-and-poisson-distributions",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.10 Normal Approximation to the Binomial and Poisson Distributions",
    "text": "3.10 Normal Approximation to the Binomial and Poisson Distributions\n\n3.10.1 Binomial Distribution Approximation\n\n\n\n\n\n\nBinomial Distribution Approximation\n\n\n\nIf X is a binomial random variable, \nZ = \\frac{X-np}{\\sqrt{np(1-p)}}\n is approximately a standard normal random variable. Consequently, probabilities computed from Z can be used to approximate probabilities for X.\nThe normal approximation to the binomial distribution is good if n is large enough relative to p, in particular, whenever\n\nnp&gt;5\\quad\\text{and}\\quad n\\left(1-p\\right)&gt;5\n\nA correction factor (known as a continuity correction) can be used to further improve the approximation.\n\n\n\n\n\n\n\nFigure 3.24: Normal approximation to the binomial distribution.\n\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nAgain consider the transmission of bits in the previous example. To judge how well the normal approximation works, assume that only n = 50 bits are to be transmitted and that the probability of an error is p = 0.1. The exact probability that 2 or fewer errors occur is\n\n\\begin{align*}\nP(X \\le 2) & = \\binom{50}{0} 0.9^{50} + \\binom{50}{1} 0.1(0.9^{49}) + \\binom{50}{2} 0.1^{2}(0.9^{48}) \\\\\n& = 0.11\n\\end{align*}\n Based on the normal approximation,\n\n\\begin{align*}\nP\\left(X&lt;2\\right) & \\approx P\\left(Z \\le \\frac{2.5 - 5}{\\sqrt{50(0.1)(0.9)}}\\right) \\\\\n& = P\\left(Z \\le -1.18\\right) = 0.12\n\\end{align*}\n\n\nR OutputR Code\n\n\n\n\n[1] 0.1117288\n\n\n[1] 0.1192964\n\n\n\n\n\nsum(dbinom(x = 0:2, size = 50, prob = 0.1))\n\npnorm(q = 2.5, mean = 5, sd = sqrt(50*0.1*0.9), lower.tail = TRUE, log.p = FALSE)\n\n\n\n\n\n\n\n\n3.10.2 Poisson Distribution Approximation\n\n\n\n\n\n\nPoisson Distribution Approximation\n\n\n\nIf X is a Poisson random variable with E(X)=\\lambda and V(X)=\\lambda,\n\nZ = \\frac{X-\\lambda}{\\sqrt{\\lambda}}\n is approximately a standard normal random variable. Consequently, probabilities computed from Z can be used to approximate probabilities for X.\nThe normal approximation to the poisson distribution is good if\n\n\\lambda &gt; 5\n\nA correction factor (known as a continuity correction) can be used to further improve the approximation.\n\n\n\n\n\n\n\n\nExample\n\n\n\nAssume that the number of contamination particles in a liter water sample follows a Poisson distribution with a mean of 1000. If a sample is analyzed, what is the probability that fewer than 950 particles are found?\nThis probability can be expressed exactly as\n\nP(X&lt;950) = \\sum_{x=0}^{950}\\frac{\\exp(-1000)\\times 1000^{x}}{x!}\n The computational difficulty is clear. The probability can be approximated as\n\n\\begin{align*}\nP\\left(X&lt;950\\right) & \\approx P\\left(Z \\le \\frac{950.5 - 1000}{\\sqrt{1000}}\\right) \\\\\n& = P\\left(Z \\le -1.57\\right) = 0.059\n\\end{align*}\n\n\nR OutputR Code\n\n\n\n\n[1] 0.05783629\n\n\n[1] 0.05875308\n\n\n\n\n\nsum(dpois(x = 0:950, lambda = 1000))\n\npnorm(q = 950.5, mean = 1000, sd = sqrt(1000), lower.tail = TRUE, log.p = FALSE)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#more-than-one-random-variable-and-independence",
    "href": "Ch03.html#more-than-one-random-variable-and-independence",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.11 More than One Random Variable and Independence",
    "text": "3.11 More than One Random Variable and Independence\n\n3.11.1 Joint Distributions\n\n\n\n\n\n\nJoint Distributions\n\n\n\nIn many experiments, more than one variable is measured. For example, suppose both the diameter and thickness of an injection-molded disk are measured and denoted by X and Y, respectively.\n\n\n\n\n\n\nFigure 3.25: Scatter diagram of diameter and thickness measurements.\n\n\n\nBecause f(x, y) determines probabilities for two random variables, it is referred to as a joint probability density function.\n\n\n\n\n\n\nFigure 3.26: Joint probability density function of x and y.\n\n\n\nFrom Figure 3.27, the probability that a part is produced in the region shown is\n\nP\\left( a &lt; X &lt; b,\\, c &lt; Y &lt; d\\right) = \\int_{a}^{b} \\int_{c}^{d}f(x,y)\\,dx\\,dy\n\n\n\n\n\n\n\nFigure 3.27: Probability of a region is the volume enclosed by f(x, y) over the region.\n\n\n\nSimilar concepts can be applied to discrete random variables.\n\n\n\n\n3.11.2 Independence\n\n\n\n\n\n\nIndependence\n\n\n\nThe random variables X_1, X_2, \\ldots , X_n are independent if\n\n\nP(X_1 \\in E_1, X_2 \\in E_2, \\ldots , X_n \\in E_n) = P(X_1 \\in E_1)P(X_2 \\in E_2) \\ldots P(X_n \\in E_n)\n\n\nfor any sets E_1, E_2, \\ldots , E_n.\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe diameter of a shaft in a storage drive is normally distributed with mean 0.2508 inch and standard deviation 0.0005 inch. The specifications on the shaft are 0.2500 \\pm 0.0015 inch. The probability that a diameter meets specifications is determined to be 0.919. What is the probability that 10 diameters all meet specifications, assuming that the diameters are independent?\nDenote the diameter of the first shaft as X_1, the diameter of the second shaft as X_2, and so forth, so that the diameter of the tenth shaft is denoted as X_{10}. The probability that all shafts meet specifications can be written as\n\n\nP(0.2485 &lt;  X_{1} &lt;  0.2515, 0.2485 &lt;  X_{2} &lt;  0.2515, \\ldots , 0.2485 &lt;  X_{10} &lt;  0.2515)\n\n\nIf the random variables are independent, the proportion of times in which we measure 10 shafts that we expect all to meet the specifications is\n\n\nP(0.2485 &lt;  X_{1} &lt;  0.2515) \\times P(0.2485 &lt;  X_{2} &lt;  0.2515) \\times \\ldots  \\times P(0.2485 &lt;  X_{10} &lt;  0.2515) = 0.919^{10} = 0.430\n\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe system shown here operates only if there is a path of functional components from left to right. The probability that each component functions is shown in the diagram. Assume that the components function or fail independently. What is the probability that the system operates?\n\n\n\n\n\n\nFigure 3.28\n\n\n\nLet C_{1} and C_{2} denote the events that components 1 and 2 are functional, respectively. For the system to operate, both components must be functional. The probaility that the system operates is \nP(C_{1},C_{1}) = P(C_{1})P(C_{2}) = (0.9)(0.95) = 0.855",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#functions-of-random-variables",
    "href": "Ch03.html#functions-of-random-variables",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.12 Functions of Random Variables",
    "text": "3.12 Functions of Random Variables\n\n\n\n\n\n\nNote\n\n\n\nLet X be a random variable (either continuous or discrete) with mean \\mu and variance \\simga^{2}, and let c be a constant. Define a new random variable Y as \nY = X + c\n Then\n\nE(Y) = E(X + c) = E(X) + E(c) = \\mu + c\n\nand\n\nV(Y) = V(X + c) = V(X) + V(c) = \\sigma^{2} + 0 = \\sigma^{2}\n\n\n\n\n3.12.1 Linear Functions of Independent Random Variables\n\n\n\n\n\n\nMean and Variance of a Linear Function: Independent Random Variables\n\n\n\nThe mean and variance of the linear function of independent random variables are\n\n\\begin{align*}\nY &= c_{0} + c_{1}X_{1} + c_{2}X_{2} + \\ldots + c_{n}X_{n} \\\\\nE(Y) &= c_{0} + c_{1}\\mu_{1} + c_{2}\\mu_{2} + \\ldots + c_{n}\\mu_{n} \\\\\nV(Y) &= c_{1}^{2}\\sigma_{1}^{2} + c_{2}^{2}\\sigma_{2}^{2} + \\ldots + c_{n}^{2}\\sigma_{n}^{2}\n\\end{align*}\n\n\n\n\n\n\n\n\n\nLinear Function of Independent Normal Random Variables\n\n\n\nLet X_{1}, X_{2}, \\ldots, X_{n} be independent, normally distributed random variables with means E(X_{i}) = \\mu_{i}, \\, i = 1,2,\\ldots, n and variances V(X_{i}) = \\sigma_{i}^{2}, \\, i = 1,2,\\ldots, n. Then the linear function\n\nY &= c_{0} + c_{1}X_{1} + c_{2}X_{2} + \\ldots + c_{n}X_{n}\n\nis normally distributed with mean\n\nE(Y) = c_{0} + c_{1}\\mu_{1} + c_{2}\\mu_{2} + \\ldots + c_{n}\\mu_{n}\n\nand variance\n$$ V(Y) = c_{1}{2}{1}^{2} + c{2}{2}{2}^{2} + + c{n}^{2}_{n}^{2}\n$$\n\n\n\n\n\n\n\n\nExample\n\n\n\nSuppose that the length X_{1} and the width X_{2} are normally and independently distributed with \\mu_{1} = 2 centimeters, \\sigma_{1}^{2} = 0.1 centimeter, \\mu_{2} = 5 centimeters, and \\sigma_{2}^{2} = 0.2 centimeter. The perimeter of the part Y = 2X_{1} + 2X_{2} is just a linear combination of the length and width were E(Y) = 14 centimeters and V(Y ) = 0.2 square centimeter, respectively. Determine the probability that the perimeter of the part exceeds 14.5 centimeters.\nSince Y is also a normally distributed random variable, so we may calculate the desired probability as follows:\n\n\nP\\left(Y&gt;14.5\\right) = P\\left(\\frac{Y-\\mu_{Y}}{\\sigma_{Y}} &gt; \\frac{14.5-14}{0.447}\\right) = P\\left(Z&gt;1.12\\right) = 0.13\n\n\nTherefore, the probability is 0.13 that the perimeter of the part exceeds 14.5 centimeters.\n\n\n\n\n3.12.2 Linear Functions of Random Variables That Are Not Independent\n\n\n\n\n\n\nCorrelation\n\n\n\nThe correlation between two random variables X_{1} and X_{2} is\n\n\\rho_{X_{1}X_{2}} = \\frac{E\\left\\{(X_{1}-\\mu_{1})(X_{2}-\\mu_{2})\\right\\}}{\\sqrt{\\sigma_{1}^{2}\\sigma_{2}^{2}}} = \\frac{Cov(X_{1}, X_{2})}{\\sqrt{\\sigma_{1}^{2}\\sigma_{2}^{2}}}\n with -1 \\le \\rho_{X_{1}X_{2}} \\le +1, and \\rho_{X_{1}X_{2}} is usually called the correlation coefficient.\n\n\n\n\n\n\n\n\nMean and Variance of a Linear Function: General Case\n\n\n\nLet X_{1}, X_{2}, \\ldots, X_{n} be random variables with means E(X_{i}) = \\mu_{i}, \\, i = 1,2,\\ldots, n and covariances Cov(X_{i}, X_{j}), \\, i,j = 1,2,\\ldots, n with i&lt;j. Then the mean of the linear combination\n\nY = c_{0} + c_{1}X_{1} + c_{2}X_{2} + \\ldots + c_{n}X_{n}\n\nis\n\nE(Y) = c_{0} + c_{1}\\mu_{1} + c_{2}\\mu_{2} + \\ldots + c_{n}\\mu_{n}\n\nand the variance is\n\nV(Y) = c_{1}^{2}\\sigma_{1}^{2} + c_{2}^{2}\\sigma_{2}^{2} + \\ldots + c_{n}^{2}\\sigma_{n}^{2} + 2 \\sum_{i&lt;j}\\sum c_{i} c_{j}Cov(X_{i}, X_{j})\n\n\n\n\n\n3.12.3 Nonlinear Functions of Independent Random Variables\n\n\n\n\n\n\nPropagation of Error Formula: Single Variable\n\n\n\nIf X has mean \\mu_{X} and variance \\sigma_{X}^{2}, the approximate mean and variance of Y = h(X) can be computed using the following result:\n\n\\begin{align*}\nE\\left(Y\\right) & = \\mu_{Y} = \\simeq h\\left(\\mu_{X}\\right)\\\\\nV\\left(Y\\right) & = \\sigma_{Y}^{2} = \\simeq \\left( \\frac{dh}{dX}\\right)^{2} \\sigma_{X}^{2}\n\\end{align*}\n\\tag{3.1}\nEngineers usually call Equation 3.1 the transmission of error or propagation of error formula.\n\n\n\n\n\n\n\n\nExample\n\n\n\nThe power P dissipated by the resistance R in an electrical circuit is given by P=I^{2}R where I, the current, is a random variable with mean \\mu_{I} = 20 amperes and standard deviation \\sigma_{I} = 0.1 amperes. The resistance R = 80 ohms is a constant. We want to find the approximate mean and standard deviation of the power. In this problem the function P = h(I) = I^{2}R, so taking the derivative dh/dI = 2IR = 2I(80) = 160I, we find that the approximate mean power is\n\n\\begin{align*}\nE\\left(P\\right) & = \\mu_{P} \\simeq h\\left(\\mu_{I}\\right) \\\\\n& = 20^{2}(80) = 32,000 \\, \\text{watts}\n\\end{align*}\n and the approximate variance of power is\n\n\\begin{align*}\nV\\left(P\\right) & = \\sigma_{P}^{2} \\simeq  \\left( \\frac{dh}{dX}\\right)^{2} \\sigma_{Y}^{2} \\\\\n& = \\left\\{106(20)\\right\\}^{2}(0.1^2) = 102,400 \\, \\text{square watts}\n\\end{align*}\n\n\n\n\n\n\n\n\n\nPropagation of Error Formula: Multiple Variables\n\n\n\nLet\n\nY = h\\left(X_{1}, X_{2}, \\ldots, X_{n}\\right)\n for independent random variables X_{i}, \\, i = 1,2, \\ldots, n each with mean \\mu_{i} and variance \\sigma_{i}^{2}, the approximate mean and variance of Y = h\\left(X_{1}, X_{2}, \\ldots, X_{n}\\right) are\n\n\\begin{align*}\nE\\left(Y\\right) & = \\mu_{Y} = \\simeq h\\left(\\mu_{1}, \\mu_{2}, \\ldots, \\mu_{n}\\right)\\\\\nV\\left(Y\\right) & = \\sigma_{Y}^{2} = \\simeq \\sum_{i=1}^{n}\\left( \\frac{dh}{dX_{i}}\\right)^{2} \\sigma_{i}^{2}\n\\end{align*}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Ch03.html#random-sample-statistics-and-the-central-limit-theorem",
    "href": "Ch03.html#random-sample-statistics-and-the-central-limit-theorem",
    "title": "3  Random Variables and Probability Distributions",
    "section": "3.13 Random Sample, Statistics, and the Central Limit Theorem",
    "text": "3.13 Random Sample, Statistics, and the Central Limit Theorem\n\n\n\n\n\n\nRandom Sample\n\n\n\nIndependent random variables X_{1}, X_{2}, \\ldots, X_{n} with the same distribution are called a random sample.\n\n\n\n\n\n\n\n\nStatistic\n\n\n\nA statistic is a function of the random variables in a random sample.\n\n\n\n\n\n\n\n\nSampling Distribution\n\n\n\nThe probability distribution of a statistic is called its sampling distribution.\n\n\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nIf X_{1}, X_{2}, \\ldots, X_{n} is a random sample of size n taken from a population with mean \\mu and variance \\sigma^2, and if \\overline{X} is the sample mean, the limiting form of the distribution of\n\nZ = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}\n as n \\rightarrow \\infty, is the standard normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) One die\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Two die\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Three die\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Five die\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Ten die\n\n\n\n\n\n\n\nFigure 3.29: Distributions of average scores from throwing dice.\n\n\n\n\n\n\n\n\n\nExample\n\n\n\nAn electronics company manufactures resistors that have a mean resistance of 100 \\Omega and a standard deviation of 10 \\Omega. Find the probability that a random sample of n = 25 resistors will have an average resistance less than 95 \\Omega.\n\n\n\n\\begin{align*}\nP\\left(\\overline{X} &lt; 95 \\right) & = P\\left(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} &lt; \\frac{95 - 100}{10/\\sqrt{25}} \\right) \\\\\nP\\left(\\overline{X} &lt; 95 \\right) & = P\\left(Z &lt; -2.5 \\right) = 0.00062\n\\end{align*}",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Montgomery, D. C., Runger, G. C., & Hubele, N. F. (2014). Engineering statistics. Wiley.",
    "crumbs": [
      "References"
    ]
  }
]